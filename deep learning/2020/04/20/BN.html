<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Speed-up inference with Batch Normalization Folding | Nathan Hubens</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Speed-up inference with Batch Normalization Folding" />
<meta name="author" content="" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How to remove the batch normalization layer to make your neural networks faster." />
<meta property="og:description" content="How to remove the batch normalization layer to make your neural networks faster." />
<link rel="canonical" href="https://nathanhubens.github.io/posts/deep%20learning/2020/04/20/BN.html" />
<meta property="og:url" content="https://nathanhubens.github.io/posts/deep%20learning/2020/04/20/BN.html" />
<meta property="og:site_name" content="Nathan Hubens" />
<meta property="og:image" content="https://nathanhubens.github.io/posts/images/BN_fold.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-20T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":""},"datePublished":"2020-04-20T00:00:00-05:00","headline":"Speed-up inference with Batch Normalization Folding","mainEntityOfPage":{"@type":"WebPage","@id":"https://nathanhubens.github.io/posts/deep%20learning/2020/04/20/BN.html"},"image":"https://nathanhubens.github.io/posts/images/BN_fold.png","description":"How to remove the batch normalization layer to make your neural networks faster.","@type":"BlogPosting","url":"https://nathanhubens.github.io/posts/deep%20learning/2020/04/20/BN.html","dateModified":"2020-04-20T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/posts/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://nathanhubens.github.io/posts/feed.xml" title="Nathan Hubens" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-164628236-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/posts/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Speed-up inference with Batch Normalization Folding | Nathan Hubens</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Speed-up inference with Batch Normalization Folding" />
<meta name="author" content="" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How to remove the batch normalization layer to make your neural networks faster." />
<meta property="og:description" content="How to remove the batch normalization layer to make your neural networks faster." />
<link rel="canonical" href="https://nathanhubens.github.io/posts/deep%20learning/2020/04/20/BN.html" />
<meta property="og:url" content="https://nathanhubens.github.io/posts/deep%20learning/2020/04/20/BN.html" />
<meta property="og:site_name" content="Nathan Hubens" />
<meta property="og:image" content="https://nathanhubens.github.io/posts/images/BN_fold.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-20T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":""},"datePublished":"2020-04-20T00:00:00-05:00","headline":"Speed-up inference with Batch Normalization Folding","mainEntityOfPage":{"@type":"WebPage","@id":"https://nathanhubens.github.io/posts/deep%20learning/2020/04/20/BN.html"},"image":"https://nathanhubens.github.io/posts/images/BN_fold.png","description":"How to remove the batch normalization layer to make your neural networks faster.","@type":"BlogPosting","url":"https://nathanhubens.github.io/posts/deep%20learning/2020/04/20/BN.html","dateModified":"2020-04-20T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://nathanhubens.github.io/posts/feed.xml" title="Nathan Hubens" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-164628236-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
<script src="https://d3js.org/d3.v4.min.js"></script>
  <script src="https://d3js.org/d3-scale-chromatic.v1.min.js"></script>
  
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/posts/">Nathan Hubens</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/posts/search/">Search</a><a class="page-link" href="/posts/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Speed-up inference with Batch Normalization Folding</h1><p class="page-description">How to remove the batch normalization layer to make your neural networks faster.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-04-20T00:00:00-05:00" itemprop="datePublished">
        Apr 20, 2020
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/posts/categories/#Deep Learning">Deep Learning</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Introduction">Introduction </a></li>
<li class="toc-entry toc-h2"><a href="#How-to-do-that-in-practice?">How to do that in practice? </a></li>
<li class="toc-entry toc-h2"><a href="#How-efficient-is-it?">How efficient is it? </a>
<ul>
<li class="toc-entry toc-h3"><a href="#VGG16">VGG16 </a></li>
<li class="toc-entry toc-h3"><a href="#Resnet50">Resnet50 </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#References">References </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-20-BN.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction">
<a class="anchor" href="#Introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Introduction</strong><a class="anchor-link" href="#Introduction"> </a>
</h2>
<p>Batch Normalization <sup id="fnref-1" class="footnote-ref"><a href="#fn-1">1</a></sup> <sup id="fnref-2" class="footnote-ref"><a href="#fn-2">2</a></sup> is a technique which takes care of normalizing the input of each layer to make the training process faster and more stable. In practice, it is an extra layer that we generally add after the computation layer and before the non-linearity.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It consists of <strong>2</strong> steps:</p>
<ol>
<li>Normalize the batch by first subtracting its mean $\mu$, then dividing it by its standard deviation $\sigma$.</li>
<li>Further scale by a factor $\gamma$ and shift by a factor $\beta$. Those are the parameters of the batch normalization layer, required in case of the network not needing the data to have a mean of <strong>0</strong> and a standard deviation of <strong>1</strong>.</li>
</ol>
$$
\Large
\begin{aligned}
&amp;\mu_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_{i}\\
&amp;\sigma_{\mathcal{B}}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(x_{i}-\mu_{\mathcal{B}}\right)^{2}\\
&amp;\widehat{x}_{i} \leftarrow \frac{x_{i}-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^{2}+\epsilon}}\\
&amp;y_{i} \leftarrow \gamma \widehat{x}_{i}+\beta \equiv \mathrm{BN}_{\gamma, \beta}\left(x_{i}\right)
\end{aligned}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Due to its efficiency for training neural networks, batch normalization is now widely used. But how useful is it at inference time?</p>
<p>Once the training has ended, each batch normalization layer possesses a specific set of $\gamma$ and $\beta$, but also $\mu$ and $\sigma$, the latter being computed using an exponentially weighted average during training. It means that during inference, the batch normalization acts as a simple linear transformation of what comes out of the previous layer, often a convolution.</p>
<p>As a convolution is also a linear transformation, it also means that both operations can be merged into a single linear transformation!</p>
<p>This would remove some unnecessary parameters but also reduce the number of operations to be performed at inference time.</p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-to-do-that-in-practice?">
<a class="anchor" href="#How-to-do-that-in-practice?" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>How to do that in practice?</strong><a class="anchor-link" href="#How-to-do-that-in-practice?"> </a>
</h2>
<p>With a little bit of math, we can easily rearrange the terms of the convolution to take the batch normalization into account.</p>
<p>As a little reminder, the convolution operation followed by the batch normalization operation can be expressed, for an input $x$, as:</p>
$$
\Large
\begin{aligned}
z &amp;=W * x+b \\
\text { out } &amp;=\gamma \cdot \frac{z-\mu}{\sqrt{\sigma^{2}+\epsilon}}+\beta
\end{aligned}
$$<p>So, if we re-arrange the $W$ and $b$ of the convolution to take the parameters of the batch normalization into account, as such:</p>
$$
\Large
\begin{aligned}
w_{\text {fold }} &amp;=\gamma \cdot \frac{W}{\sqrt{\sigma^{2}+\epsilon}} \\
b_{\text {fold }} &amp;=\gamma \cdot \frac{b-\mu}{\sqrt{\sigma^{2}+\epsilon}}+\beta
\end{aligned}
$$<p>We can remove the batch normalization layer and still have the same results!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash">
    <svg class="octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg>
    <strong>Note: </strong>Usually, you donâ€™t have a bias in a layer preceding a batch normalization layer. It is useless and a waste of parameters as any constant will be canceled out by the batch normalization.
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-efficient-is-it?">
<a class="anchor" href="#How-efficient-is-it?" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>How efficient is it?</strong><a class="anchor-link" href="#How-efficient-is-it?"> </a>
</h2>
<p>We will try for <strong>2</strong> common architectures:</p>
<ol>
<li>VGG16 with batch norm</li>
<li>ResNet50</li>
</ol>
<p>Just for the demonstration, we will use ImageNette dataset and PyTorch. Both networks will be trained for <strong>5</strong> epochs and what changes in terms of parameter number and inference time.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="VGG16">
<a class="anchor" href="#VGG16" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>VGG16</strong><a class="anchor-link" href="#VGG16"> </a>
</h3>
<p>Letâ€™s start by training VGG16 for <strong>5</strong> epochs (the final accuracy doesnâ€™t matter):</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.985012</td>
      <td>3.945934</td>
      <td>0.226497</td>
      <td>00:31</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.868819</td>
      <td>1.620619</td>
      <td>0.472611</td>
      <td>00:31</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.574975</td>
      <td>1.295385</td>
      <td>0.576815</td>
      <td>00:31</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.305211</td>
      <td>1.161460</td>
      <td>0.617325</td>
      <td>00:32</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1.072395</td>
      <td>0.955824</td>
      <td>0.684076</td>
      <td>00:32</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then show its number of parameters:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Total parameters : 134,309,962
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can get the initial inference time by using the <code>%%timeit</code> magic command:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>it
<span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>2.77 ms Â± 1.65 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So now if we apply batch normalization folding, we have:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Total parameters : 134,301,514
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>it
<span class="n">folded_model</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>2.41 ms Â± 2.49 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So <strong>8448</strong> parameters removed and even better, almost <strong>0.4 ms</strong> faster inference! Most importantly, this is completely lossless, there is absolutely no change in terms of performance:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">folded_learner</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[0.9558241, tensor(0.6841)]</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Letâ€™s see how it behaves in the case of Resnet50!</p>
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Resnet50">
<a class="anchor" href="#Resnet50" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Resnet50</strong><a class="anchor-link" href="#Resnet50"> </a>
</h3>
<p>Same, we start by training it for <strong>5</strong> epochs:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>2.076416</td>
      <td>2.491038</td>
      <td>0.246624</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.696750</td>
      <td>1.517581</td>
      <td>0.489427</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.313028</td>
      <td>1.206347</td>
      <td>0.606115</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.057600</td>
      <td>0.890211</td>
      <td>0.716943</td>
      <td>00:21</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.828224</td>
      <td>0.793130</td>
      <td>0.740892</td>
      <td>00:19</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The initial amount of parameters is:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Total parameters : 23,528,522
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And inference time is:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>it
<span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>6.17 ms Â± 13.3 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>After using batch normalization folding, we have:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Total parameters : 23,501,962
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>it
<span class="n">final_model</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>4.47 ms Â± 8.97 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So now, we have <strong>26,560</strong> parameters removed and even more impressive, an inference time reduce by <strong>1.7ms</strong>! And still without any drop in performance.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">final_learner</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[0.7931296, tensor(0.7409)]</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<p><span style="font-size:larger;">So if we can reduce the inference time and the number of parameters of our models without enduring any drop in performance, why shouldnâ€™t we always do it?</span></p>
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>I hope that this blog post helped you! Feel free to give me feedback or ask me questions is something is not clear enough.</strong></p>
<p>Code available at <a href="https://github.com/nathanhubens/fasterai">this address!</a></p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="References">
<a class="anchor" href="#References" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>References</strong><a class="anchor-link" href="#References"> </a>
</h2>
<ul>
<li>
<div class="footnotes"><p id="fn-1">1. <a href="https://arxiv.org/pdf/1502.03167.pdf">The Batch Normalization paper</a><a href="#fnref-1" class="footnote footnotes">â†©</a></p></div> </li>
<li><div class="footnotes"><p id="fn-2">2. <a href="https://www.youtube.com/watch?v=tNIpEZLv_eg&amp;t=1s">DeepLearning.ai Batch Normalization Lesson</a><a href="#fnref-2" class="footnote footnotes">â†©</a></p></div></li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="/posts"
        issue-term="title"
        label="blogpost-comment"
        theme="icy-dark"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/posts/deep%20learning/2020/04/20/BN.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/posts/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/posts/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/posts/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name"></li>
          <li><a class="u-email" href="mailto:nathan.hubens@gmail.com">nathan.hubens@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/nathanhubens" target="_blank" title="nathanhubens"><svg class="svg-icon grey"><use xlink:href="/posts/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/HubensN" target="_blank" title="HubensN"><svg class="svg-icon grey"><use xlink:href="/posts/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
