<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Neural Network Pruning | Nathan Hubens</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Neural Network Pruning" />
<meta name="author" content="" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How to make your network smaller and faster by removing useless parameters" />
<meta property="og:description" content="How to make your network smaller and faster by removing useless parameters" />
<link rel="canonical" href="https://nathanhubens.github.io/posts/deep%20learning/2020/05/22/pruning.html" />
<meta property="og:url" content="https://nathanhubens.github.io/posts/deep%20learning/2020/05/22/pruning.html" />
<meta property="og:site_name" content="Nathan Hubens" />
<meta property="og:image" content="https://nathanhubens.github.io/posts/images/granularity.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-22T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":""},"description":"How to make your network smaller and faster by removing useless parameters","@type":"BlogPosting","headline":"Neural Network Pruning","url":"https://nathanhubens.github.io/posts/deep%20learning/2020/05/22/pruning.html","datePublished":"2020-05-22T00:00:00-05:00","dateModified":"2020-05-22T00:00:00-05:00","image":"https://nathanhubens.github.io/posts/images/granularity.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://nathanhubens.github.io/posts/deep%20learning/2020/05/22/pruning.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/posts/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://nathanhubens.github.io/posts/feed.xml" title="Nathan Hubens" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-164628236-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/posts/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Neural Network Pruning | Nathan Hubens</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Neural Network Pruning" />
<meta name="author" content="" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How to make your network smaller and faster by removing useless parameters" />
<meta property="og:description" content="How to make your network smaller and faster by removing useless parameters" />
<link rel="canonical" href="https://nathanhubens.github.io/posts/deep%20learning/2020/05/22/pruning.html" />
<meta property="og:url" content="https://nathanhubens.github.io/posts/deep%20learning/2020/05/22/pruning.html" />
<meta property="og:site_name" content="Nathan Hubens" />
<meta property="og:image" content="https://nathanhubens.github.io/posts/images/granularity.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-22T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":""},"description":"How to make your network smaller and faster by removing useless parameters","@type":"BlogPosting","headline":"Neural Network Pruning","url":"https://nathanhubens.github.io/posts/deep%20learning/2020/05/22/pruning.html","datePublished":"2020-05-22T00:00:00-05:00","dateModified":"2020-05-22T00:00:00-05:00","image":"https://nathanhubens.github.io/posts/images/granularity.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://nathanhubens.github.io/posts/deep%20learning/2020/05/22/pruning.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://nathanhubens.github.io/posts/feed.xml" title="Nathan Hubens" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-164628236-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/posts/">Nathan Hubens</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/posts/search/">Search</a><a class="page-link" href="/posts/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Neural Network Pruning</h1><p class="page-description">How to make your network smaller and faster by removing useless parameters</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-05-22T00:00:00-05:00" itemprop="datePublished">
        May 22, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      10 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/posts/categories/#Deep Learning">Deep Learning</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#The-curse-of-racing-for-high-performance">The curse of racing for high performance </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Do-we-really-need-a-sledgehammer-to-crack-a-nut-?">Do we really need a sledgehammer to crack a nut ? </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Pruning">Pruning </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Granularity">Granularity </a></li>
<li class="toc-entry toc-h3"><a href="#Scheduling">Scheduling </a></li>
<li class="toc-entry toc-h3"><a href="#Criteria">Criteria </a></li>
<li class="toc-entry toc-h3"><a href="#Evaluation">Evaluation </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#References">References </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-05-22-pruning.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-curse-of-racing-for-high-performance">
<a class="anchor" href="#The-curse-of-racing-for-high-performance" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>The curse of racing for high performance</strong><a class="anchor-link" href="#The-curse-of-racing-for-high-performance"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>While deep learning models are getting better in terms of performance, they also tend to get bigger and more expensive to compute. Until recently, it can seem that state-of-the-art models were achieved by using the good ol' <strong><em>“Stack more layers !”</em></strong> property. Indeed, if you take a look at the history of state-of-the-art models on ImageNet, you will notice that each year, the new best results were achieved by using a deeper network.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It seems that we are obsessed by getting the best results as possible, leading to models that can involve hundreds of millions or parameters ! But what's the point of having a top-tier performing network if we cannot use it ?</p>
<p><br></p>
<p style="font-size: 15px"><i>There <b>has</b> to be a better way to solve our tasks...</i></p>
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>After all, as the philosopher William of Occam argued in his principle named after him, the <a href="http://math.ucr.edu/home/baez/physics/General/occam.html">Occam's Razor</a>:</p>
<blockquote>
<p>“Pluralitas non est ponenda sine necessitate.”</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Or in other words:</p>
<blockquote>
<p>“Simpler solutions should be favored over more complex ones.”</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One of the ways to do so is to use a more parameter efficient architecture. There is a very active research going on in that field, and we can state some notable architectures that are competitive with their parameter-heavy equivalents (<a href="https://arxiv.org/pdf/1602.07360.pdf">SqueezeNet</a><sup id="fnref-1" class="footnote-ref"><a href="#fn-1">1</a></sup>, <a href="https://arxiv.org/pdf/1704.04861.pdf">MobileNet</a><sup id="fnref-2" class="footnote-ref"><a href="#fn-2">2</a></sup>, <a href="https://arxiv.org/pdf/1610.02357.pdf">Xception</a><sup id="fnref-3" class="footnote-ref"><a href="#fn-3">3</a></sup>, <a href="https://arxiv.org/pdf/1905.11946.pdf">EfficientNet</a><sup id="fnref-4" class="footnote-ref"><a href="#fn-4">4</a></sup>,...).</p>
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Do-we-really-need-a-sledgehammer-to-crack-a-nut-?">
<a class="anchor" href="#Do-we-really-need-a-sledgehammer-to-crack-a-nut-?" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Do we really need a sledgehammer to crack a nut ?</strong><a class="anchor-link" href="#Do-we-really-need-a-sledgehammer-to-crack-a-nut-?"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What if I told you that you can still use your favorite architecture, but in a more efficient way ? After all, neural networks are meant to be used either on ressource-constrained environments (mobile phones, autonomous cars, drones, ...) or to run on servers, so in both case, any gain in computation or memory storage would be beneficial.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Recently, there has been a very interesting paper by <a href="https://arxiv.org/pdf/1803.03635.pdf">Frankle and Carbin</a><sup id="fnref-5" class="footnote-ref"><a href="#fn-5">5</a></sup>, where they introduce the <em>lottery ticket hypothesis</em>. This hypothesis states that, in a network, only a subset of the parameters are needed to achieve an optimal performance. The whole difficulty is then to find that particular subnetwork; the authors used a pruning technique consisting of pruning the trained model, then “rewinding” the initialization of remaining weights to their original initialization. Only that subnetwork, with that specific set of initialized weights is able to achieve the same level of accuracy as the entire network.</p>
<p>This discovery has huge implications, as it would imply that the only advantage of using parameter-heavy networks is to provide more initialization configuration and thus, more chance to get those “winning tickets”.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Pruning">
<a class="anchor" href="#Pruning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Pruning</strong><a class="anchor-link" href="#Pruning"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <a href="https://en.wikipedia.org/wiki/Synaptic_pruning">inspiration</a> behind neural network pruning is taken from how our own brain evolves during our life. Indeed, between our birth and adulthood, the amount of synapses (the structures that allows the neurons to transmit an electrical or chemical signal to another neuron) greatly varies. Our brain experience a large amount of growth during infancy, then basically follows a “use it or lose it” process. It results in a synaptic pruning which will remove any synapse that is not needed, in order to reach an optimal amount depending on our needs.</p>
<p><img src="/posts/images/copied_from_nb/images/pruning/synapses.png" alt="'Synapses'" title="Source: Rethinking the Brain: New Insights into Early Development"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the case of neural networks, the principle of pruning is to remove network connections that are considered <em>unimportant</em> to keep the network performance unchanged. Pruning is actually a quite old idea (like most ideas of deep learning) but that is an active field of research nowadays.</p>
<p>It dates back to 1990s namely, with most popular work at that time being <a href="https://papers.nips.cc/paper/250-optimal-brain-damage.pdf">Optimal Brain Damage</a> <sup id="fnref-6" class="footnote-ref"><a href="#fn-6">6</a></sup> and  <a href="https://papers.nips.cc/paper/749-optimal-brain-surgeon-extensions-and-performance-comparisons.pdf">Optimal Brain Surgeon</a> <sup id="fnref-7" class="footnote-ref"><a href="#fn-7">7</a></sup>. Pruning has been popularized by <a href="https://arxiv.org/abs/1506.02626">Han et.al.</a> <sup id="fnref-8" class="footnote-ref"><a href="#fn-8">8</a></sup> with their 2015 paper.</p>
<p><img src="/posts/images/copied_from_nb/images/pruning/pruning.png" alt="'Pruning'"></p>
<p>Pruning thus consists of inducing sparsity in the weights of the network.</p>
<blockquote>
<p>“Every block of stone has a statue inside it and it is the task of the sculptor to discover it.” - <strong>Michelangelo</strong></p>
</blockquote>
<p>As Michelangelo and his blocks of stone, we will carve our neural networks to bring their beauty out of them, or until we make them as sparse as possible while preserving their original performance.</p>
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Granularity">
<a class="anchor" href="#Granularity" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Granularity</strong><a class="anchor-link" href="#Granularity"> </a>
</h3>
<p>Neural network pruning can come in many fashion, represented in the image below:</p>
<p><img src="/posts/images/copied_from_nb/images/pruning/granularity.png" alt="'Synapses'" title="Inspired by Mao et al., Exploring the Regularity of Sparse Structure in Convolutional Neural Networks"></p>
<p>You can either be very precise and remove each weight independently or remove bigger chunks at a time. The more fine-grained (or <em>unstructured</em>) the pruning, the more precise the process will be, but the more difficult it will be to get any acceleration. On the other hand, removing bigger chunks at a time (<em>structured pruning</em>) will be less accurate but will make life easier for any sparse matrices computation libraries. So granularity of pruning will be a trade-off between precision and acceleration.</p>
<p><br></p>
<p style="font-size: 15px"><i>I guess it's a matter of preference, are you more a <b>cubist</b> or <b>high renaissance</b> deep learning artist ?</i></p>
<p><br></p>
<p>Apart from the granularity of pruning, you also have to choose <strong>when</strong> you will remove the weights.</p>
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Scheduling">
<a class="anchor" href="#Scheduling" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Scheduling</strong><a class="anchor-link" href="#Scheduling"> </a>
</h3>
<p>The timing and scheduling that you will adopt to prune your network will highly impact its final performance.</p>
<p>The three most commonly used schedulings are:</p>
<ul>
<li><strong>One-shot Pruning</strong></li>
<li><strong>Iterative Pruning</strong></li>
<li><strong>Automated Gradual Pruning</strong></li>
</ul>
<p>The most basic idea is to start from a trained model, prune it to the desired sparsity, then optionally fine-tune the network to accomodate from the removal of some of its parameters. This technique is known as <strong><em>One-shot Pruning</em></strong>. However, a simple change to that technique is able to provide way better results. The idea is simply to perform the pruning phase over several steps, all followed by some fine-tuning. That technique is called <strong><em>Iterative Pruning</em></strong> and, while leading to better results, can sometimes be very time-consuming and computation intensive, especially if the number of parameters removed at each iteration is low. There has also been <a href="https://arxiv.org/abs/1710.01878">some research</a><sup id="fnref-9" class="footnote-ref"><a href="#fn-9">9</a></sup> in incorporating weight pruning directly <strong>inside</strong> of the training step, periodically removing the weights. This technique is called <strong><em>Automated Gradual Pruning</em></strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><figure>
  
    <img class="docimage" src="/posts/images/copied_from_nb/images/pruning/schedules.png" alt="" style="max-width: 680px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the case of Automated Gradual Pruning, the schedule proposed by the authors is the following:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><figure>
  
    <img class="docimage" src="/posts/images/copied_from_nb/images/pruning/gradual.png" alt="" style="max-width: 500px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This schedule leads to an important pruning early in the training, then slowly decreasing as the training progresses.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Criteria">
<a class="anchor" href="#Criteria" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Criteria</strong><a class="anchor-link" href="#Criteria"> </a>
</h3>
<p>Now that we know how and when to remove our parameters, we have to know <strong>which</strong> ones to choose.</p>
<p>There exists many ways to evaluate weights importance but the two most common ways are:</p>
<ul>
<li><strong>Weight Magnitude Pruning</strong></li>
<li><strong>Gradient Magnitude Pruning</strong></li>
</ul>
<p>While being extremely simple, weight magnitude pruning has been found to be very effective. It simply consists of computing the $L1$-norm, i.e $\sum_{i}\left|x_{i}\right|$, of the weights (or group/kernel/filter depending on the granularity), and to remove the ones with the lowest values. In the case of gradient magnitude pruning, the only change is that we will multiply our weights by their corresponding gradients before computing the $L1$-norm on the result.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Those criteria can be evaluated <strong>locally</strong>, i.e. each channel is pruned until the desired sparsity is reached, resulting in equally sparse layers, or <strong>globally</strong>, i.e. we evaluate the weights over the whole network, resulting in a sparse network, but with layers having different sparsity values.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Evaluation">
<a class="anchor" href="#Evaluation" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Evaluation</strong><a class="anchor-link" href="#Evaluation"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In order to report how well a pruning technique is doing, you need metrics to evaluate it.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To avoid any ambiguity in metrics used, <a href="https://arxiv.org/pdf/2003.03033.pdf">Davis Blalock and Jose Javier Gonzalez Ortiz</a><sup id="fnref-10" class="footnote-ref"><a href="#fn-10">10</a></sup> have proposed a <a href="https://github.com/JJGO/shrinkbench/tree/master/">library</a> to unify the way we report metrics. More specifically they propose to report:</p>
<ul>
<li>
<p><strong>Compression Ratio</strong>, which should be computed as: <code>Compression Ratio = total_params/nonzero_params</code>, with <code>total_params</code> being the original number of parameters in the network and <code>nonzero_params</code> the number of non-zero weights after pruning.</p>
</li>
<li>
<p><strong>Theoretical Speed-Up</strong>, which should be computed as: <code>Speed-up = total_flops/nonzero_flops</code>, with <code>total_flops</code> being the amount of FLOPs in the original model and <code>nonzero_flops</code> the amount of FLOPs of the remaining non-zero weights.</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is important to report both metrics as the speed-up greatly depends on <strong>where</strong> in the network the pruning is performed. Indeed, for a same compression ratio, two similar architectures can have widely different speed-up values. This is because the FLOPs of the convolution operation highly depend on the size of their input dimension, which varies along the network. Most of parameters are usually contained towards the end of the network while most of the computation is performed in early layers, reason why early downsampling is widely used.</p>
<p>The graph below show how the number of parameters and FLOPs evolve in the VGG16 network:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><figure>
  
    <img class="docimage" src="/posts/images/copied_from_nb/images/pruning/VGG2.png" alt="" style="max-width: 600px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What it shows is that, the last 3 layers hold $48\%$ of the parameters while being only responsible of $9\%$ of the total FLOPs in the network. For that reason, in order to see the same speed-up improvement, you will need to remove a lot more parameters in late layers than you would need in early layers.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Another problem remaining is that the reported speed-up is a <strong>theoretical value</strong>. It means that you'll never observe such a speed-up in reality, especially because common deep learnings do not support acceleration for sparse matrices, or that it requires dedicated hardware. The easiest way to make sure that you will get an inference speed improvement is to physically remove the weights (can only be done for <a href="https://arxiv.org/pdf/1608.08710.pdf">entire filters</a> <sup id="fnref-11" class="footnote-ref"><a href="#fn-11">11</a></sup>), you don't need to take care of sparse matrices computations if you don't have the matrix anymore! The way to perform that operation for a layer $i$ is the following:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/posts/images/copied_from_nb/images/pruning/filter_pruning.png" alt="'Filter Pruning'"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As you remove a whole filter, it's not really introducing sparsity in the network as you now change an hyperparameter (number of filter in a layer). Moreover, if you decide to prune a single filter in layer $i$, it means that the corresponding feature map won't exist anymore. Thus, in the layer $i+1$, the kernels corresponding to the deleted feature maps have to be removed. So, pruning a filter saves parameters and computations both in the current layer and in the following one !</p>
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>That's all ! Thank you for reading, I hope that you found this little tour over neural network pruning interesting and, more importantly, useful.</strong></p>
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p style="font-size: 15px"><i>If you notice any mistake or improvement that can be done, please contact me ! If you found that post useful, please consider citing it as:</i></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">

<pre><code>@article{hubens2020pruning,
  title   = "Neural Network Pruning",
  author  = "Hubens, Nathan",
  journal = "nathanhubens.github.io",
  year    = "2020",
  url     = "https://nathanhubens.github.io/posts/deep%20learning/2020/05/22/pruning.html"
}</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="References">
<a class="anchor" href="#References" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>References</strong><a class="anchor-link" href="#References"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>
<p></p>
<div class="footnotes"><p id="fn-1">1. <a href="http://arxiv.org/abs/1803.03635">Howard A. et al., SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;1MB model size. ICLR, 2017</a><a href="#fnref-1" class="footnote footnotes">↩</a></p></div>
</li>
<li>
<p></p>
<div class="footnotes"><p id="fn-2">2. <a href="http://arxiv.org/abs/1704.04861">Forrest N. Iandola et al., MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. CoRR, abs/1704.04861, 2017</a><a href="#fnref-2" class="footnote footnotes">↩</a></p></div>
</li>
<li>
<p></p>
<div class="footnotes"><p id="fn-3">3. <a href="https://doi.org/10.1109/CVPR.2017.195">Chollet F., Xception: Deep Learning with Depthwise Separable Convolutions. CVPR, 2017</a><a href="#fnref-3" class="footnote footnotes">↩</a></p></div>
</li>
<li>
<p></p>
<div class="footnotes"><p id="fn-4">4. <a href="http://proceedings.mlr.press/v97/tan19a.html">Mingxing T. and Quoc V. Le, EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. ICML, 2019</a><a href="#fnref-4" class="footnote footnotes">↩</a></p></div>
</li>
<li>
<p></p>
<div class="footnotes"><p id="fn-5">5. <a href="http://arxiv.org/abs/1803.03635">Frankle, J. &amp; Carbin, M., The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. ICLR, 2019</a><a href="#fnref-5" class="footnote footnotes">↩</a></p></div>
</li>
<li>
<p></p>
<div class="footnotes"><p id="fn-6">6. <a href="http://papers.nips.cc/paper/250-optimal-brain-damage.pdf">LeCun Y. and Denker J. and Solla S., Optimal Brain Damage, NeurIPS, 1990</a><a href="#fnref-6" class="footnote footnotes">↩</a></p></div>
</li>
<li>
<p></p>
<div class="footnotes"><p id="fn-7">7. <a href="http://papers.nips.cc/paper/749-optimal-brain-surgeon-extensions-and-performance-comparisons.pdf">Hassibi, Babak and David G. Stork and Wolff, Gregory, Optimal Brain Surgeon: Extensions and performance comparisons, NeurIPS, 1993</a><a href="#fnref-7" class="footnote footnotes">↩</a></p></div>
</li>
<li>
<p></p>
<div class="footnotes"><p id="fn-8">8. <a href="http://arxiv.org/abs/1506.02626">Han S. et al., Learning both Weights and Connections for Efficient Neural Networks, NeurIPS, 2015</a><a href="#fnref-8" class="footnote footnotes">↩</a></p></div>
</li>
<li>
<p></p>
<div class="footnotes"><p id="fn-9">9. <a href="https://arxiv.org/abs/1710.01878">Zhu, M. &amp; Gupta, S. To prune, or not to prune: exploring the efficacy of pruning for model compression. ICLR, 2018</a><a href="#fnref-9" class="footnote footnotes">↩</a></p></div>
</li>
<li>
<p></p>
<div class="footnotes"><p id="fn-10">10. <a href="http://arxiv.org/abs/1803.03635">Blalock D.,Gonzalez Ortiz J.,Frankle J., and Guttag J., What is the state of neural network pruning?. MLSys, 2020</a><a href="#fnref-10" class="footnote footnotes">↩</a></p></div>
</li>
<li>
<p></p>
<div class="footnotes"><p id="fn-11">11. <a href="https://arxiv.org/pdf/1608.08710.pdf">Hao Li et al., Pruning Filters for Efficient ConvNets, ICLR, 2017</a><a href="#fnref-11" class="footnote footnotes">↩</a></p></div>
</li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="/posts"
        issue-term="title"
        label="blogpost-comment"
        theme="icy-dark"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/posts/deep%20learning/2020/05/22/pruning.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/posts/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/posts/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/posts/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name"></li>
          <li><a class="u-email" href="mailto:nathan.hubens@gmail.com">nathan.hubens@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/nathanhubens" title="nathanhubens"><svg class="svg-icon grey"><use xlink:href="/posts/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/HubensN" title="HubensN"><svg class="svg-icon grey"><use xlink:href="/posts/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
