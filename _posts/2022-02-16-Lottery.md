---
keywords: fastai
description: How to find winning tickets in your neural network
title: Winning at the Lottery with fasterai
toc: true
badges: false
categories: [Deep Learning]
comments: true
image: images/LTH.png
hide: false
nb_path: _notebooks/2022-02-16-Lottery.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-02-16-Lottery.ipynb
-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction"><strong>Introduction</strong><a class="anchor-link" href="#Introduction"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Creating sparse neural networks is a very hot topic at the moment. It is believed to make them <strong>smaller</strong>, <strong>faster</strong> and with <strong>better generalization</strong> capabilities{% fn 1 %}. For a long time however, it was believed that sparse networks were difficult to train. The traditional way of getting them was therefore to first train a dense network to convergence, then prune it to make it sparse, eventually fine-tuning it a tiny bit more to recover performance. However, some recent research has shown that not only it was possible to train sparse networks, but also that they may outperform their more-parameterized, dense, counterpart. The paper that initiated this trend talks about "lottery tickets", that may be hidden in neural networks{% fn 2 %}. In this blog post, we are going to explain what they are and how we can find them, with the help of <a href="https://docs.fast.ai">fastai</a>, and more particularly <a href="https://nathanhubens.github.io/fasterai/">fasterai</a>, a library to create smaller and faster neural networks that we created.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Lottery-Ticket-Hypothesis"><strong>Lottery Ticket Hypothesis</strong><a class="anchor-link" href="#Lottery-Ticket-Hypothesis"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's first introduce what The Lottery Ticket Hypothesis is for those who may have never heard about it. It is a fascinating characteristic of neural networks that has been discovered by Frankle and Carbin in 2019{% fn 2 %}. The gist of this hypothesis can be phrased as the following:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><p>In a neural network, there exists a subnetwork that can be trained to at least the same accuracy and in at most the same training time as the whole network. The only condition being that both this sub- and the complete networks start from the same initial conditions.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This subnetwork, called the "winning ticket" (as it is believed to have won at the initialization lottery), can be found by using pruning on the network, removing useless connections.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The steps to unveil this winning ticket are:</p>
<ol>
<li>Get a freshly initialized network, possessing a set of weights $W_0$</li>
<li>Train it for a certain amount $T$ of iterations, giving us the network with weights $W_T$</li>
<li>Prune a portion of the smallest weights, i.e. the weights that possess the lowest $l_1$-norm, giving us the network with weights $W_T \odot m$, with $m$ being a binary mask constituted of $0$ for weights we want to remove and $1$ for those we want to keep.</li>
<li>Reinitialize the remaining weights to their original value, i.e. their value at step 1), giving us the network with weights $W_0 \odot m$.</li>
<li>Stop if target sparsity is reached or go back to step 2)</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/posts/images/copied_from_nb/images/LTH/test2.gif" alt="Alt Text"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will conduct this tutorial by using a ResNet-18 architecture, trained on <a href="https://github.com/fastai/imagenette">Imagenette</a>, a subpart of Imagenet using only 10 classes.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We first want a baseline of the complete model that we can then compare to.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">resnet18</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's save the weights of this model, so that we can be sure to start from the exact same network in our further experiments.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">initial_weights</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As this is our baseline, it will not be pruned. Thus, this model corresponds to $W_T$, with $T$ chosen to be $5$ epochs. So let's train it and report the final accuracy.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.536754</td>
      <td>1.709699</td>
      <td>0.481529</td>
      <td>00:11</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.254531</td>
      <td>1.314451</td>
      <td>0.578089</td>
      <td>00:11</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.116412</td>
      <td>1.168404</td>
      <td>0.634904</td>
      <td>00:11</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.023481</td>
      <td>1.156428</td>
      <td>0.633376</td>
      <td>00:11</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.946494</td>
      <td>0.998459</td>
      <td>0.677962</td>
      <td>00:11</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>After training, our baseline network is $68\%$ accurate at discriminating between the 10 classes of our validation set.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><p>Can we please find winning tickets now ?</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We have already shown in a <a href="https://nathanhubens.github.io/posts/deep%20learning/2020/08/17/FasterAI.html#Sparsifying">previous blog</a> post how to prune a network with fasterai. As a quick reminder, this can be done by using the <code>SparsifyCallback</code> callback during training.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The only things to specify in the callback are:</p>
<ul>
<li><code>end_sparsity</code>, the target final level of sparsity in the network</li>
<li><code>granularity</code>, the shape of parameters to remove, e.g. <code>weight</code> or <code>filter</code></li>
<li><code>method</code>, either prune the weights in each layer separately (<code>local</code>) or in the whole network (<code>global</code>)</li>
<li><code>criteria</code>, i.e. how to score the importance of parameters to remove</li>
<li><code>schedule</code>, i.e. when pruning is applied during training</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the original paper, authors discover tickets using an Iterative Magnitude Pruning (IMP), meaning that the pruning is performed <em>iteratively</em>, with a <em>criteria</em> based on magnitude, i.e. the $l_1$-norm of weights. Authors also specify that they remove individual <em>weights</em>, comparing them across the network <em>globally</em>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Luckily for us, all of these were already available in fasterai! We now know most of the parameters of our callback:
<code>SparsifyCallback(end_sparsity, granularity='weight', method='global', criteria=large_final, schedule=iterative)</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We are all set then ! Well almost... If you remember correctly the 5 steps presented earlier, we need to keep track of the set of weights $W_0$, at initialization. We also need to reset our weights to their initial value after each pruning step.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In fasterai this can be done by:</p>
<ul>
<li>passing the <code>lth</code> argument to <code>True</code>. Behind the hood, <strong>fasterai</strong> will save the initial weights of the model and reset them after each pruning step</li>
<li>Optionnally setting a <code>start_epoch</code>, which affects at which epoch the pruning process will start.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's recreate the exact same model as the one we used for baseline.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">resnet18</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">initial_weights</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;All keys matched successfully&gt;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In fasterai, the <code>iterative</code> schedule has 3 steps by default, which can easily be changed but we'll stick with it for our experiments.</p>
<p>We'll thus have 3 rounds of pruning, and that our network will therefor be reset 3 times. As we want the network to be trained for $T=5$ epochs at each round, this means that the total epochs over which pruning will occur is $3 \times 5 = 15$ epochs.</p>
<p><strong>But</strong> before performing any round of pruning, there is first a pretraining phase of $T$ epochs. The total number of epochs is then $20$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's train this bad boy and see what happens !</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sp_cb</span> <span class="o">=</span> <span class="n">SparsifyCallback</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="s1">&#39;global&#39;</span><span class="p">,</span> <span class="n">large_final</span><span class="p">,</span> <span class="n">iterative</span><span class="p">,</span> <span class="n">start_epoch</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">lth</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">cbs</span><span class="o">=</span><span class="n">sp_cb</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Pruning of weight until a sparsity of 50%
Saving Weights at epoch 0
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.541520</td>
      <td>1.568734</td>
      <td>0.501911</td>
      <td>00:11</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.258532</td>
      <td>1.628220</td>
      <td>0.508790</td>
      <td>00:11</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.111838</td>
      <td>1.292680</td>
      <td>0.596688</td>
      <td>00:11</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.024304</td>
      <td>1.385538</td>
      <td>0.581146</td>
      <td>00:11</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.930883</td>
      <td>1.041547</td>
      <td>0.672102</td>
      <td>00:11</td>
    </tr>
    <tr>
      <td>5</td>
      <td>1.330930</td>
      <td>1.395270</td>
      <td>0.520510</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>6</td>
      <td>1.141437</td>
      <td>1.135004</td>
      <td>0.620637</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>7</td>
      <td>1.040761</td>
      <td>1.267395</td>
      <td>0.581656</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.952175</td>
      <td>1.272328</td>
      <td>0.594650</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.909871</td>
      <td>1.207141</td>
      <td>0.629554</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>10</td>
      <td>1.235558</td>
      <td>1.197264</td>
      <td>0.598217</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>11</td>
      <td>1.042131</td>
      <td>1.067109</td>
      <td>0.658854</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.927392</td>
      <td>0.977499</td>
      <td>0.673376</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.888816</td>
      <td>0.916399</td>
      <td>0.699873</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.800480</td>
      <td>0.774320</td>
      <td>0.743439</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>15</td>
      <td>1.052142</td>
      <td>1.027188</td>
      <td>0.665223</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.921996</td>
      <td>0.945266</td>
      <td>0.694268</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.831712</td>
      <td>0.868593</td>
      <td>0.717452</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.812539</td>
      <td>1.016729</td>
      <td>0.673376</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.764737</td>
      <td>0.859072</td>
      <td>0.725860</td>
      <td>00:19</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Sparsity at the end of epoch 0: 0.00%
Sparsity at the end of epoch 1: 0.00%
Sparsity at the end of epoch 2: 0.00%
Sparsity at the end of epoch 3: 0.00%
Sparsity at the end of epoch 4: 0.00%
Resetting Weights to their epoch 0 values
Sparsity at the end of epoch 5: 16.67%
Sparsity at the end of epoch 6: 16.67%
Sparsity at the end of epoch 7: 16.67%
Sparsity at the end of epoch 8: 16.67%
Sparsity at the end of epoch 9: 16.67%
Resetting Weights to their epoch 0 values
Sparsity at the end of epoch 10: 33.33%
Sparsity at the end of epoch 11: 33.33%
Sparsity at the end of epoch 12: 33.33%
Sparsity at the end of epoch 13: 33.33%
Sparsity at the end of epoch 14: 33.33%
Resetting Weights to their epoch 0 values
Sparsity at the end of epoch 15: 50.00%
Sparsity at the end of epoch 16: 50.00%
Sparsity at the end of epoch 17: 50.00%
Sparsity at the end of epoch 18: 50.00%
Sparsity at the end of epoch 19: 50.00%
Final Sparsity: 50.00
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As can be seen from the verbose below training results, the weights are reset to their original value every 5 epochs. This can also be observed when looking at the accuracy, which drops after each pruning round.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>After each round, the sparsity level is increased, meaning that the binary mask $m$ in $W_T \odot m$ has more and more zeroes as the training goes.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The last round, performed at a constant sparsity level of $50\%$, is able to reach $72\%$ of accuracy in 5 epochs, which is better than our baseline !</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Lottery-Ticket-Hypothesis-with-Rewinding"><strong>Lottery Ticket Hypothesis with Rewinding</strong><a class="anchor-link" href="#Lottery-Ticket-Hypothesis-with-Rewinding"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>However, authors noticed that this IMP procedure may fail on deeper networks{% fn 3 %}, they thus propose to weaken the original Lottery Ticket Hypothesis, making the network to be reset to weights early in training instead of at initialization, i.e. our step 4) now resets the weights to $W_t \odot m$ with $t&lt;T$. Such a subnetwork is no longer called a "winning" ticket, but a "matching" ticket. In this case, the regular LTH is just the particular case of $t=0$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In fasterai, this can be done by changing the <code>rewind_epoch</code> value to the epoch you want your weights to be reset to, everything else stays the same. Let's try this !</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">resnet18</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">initial_weights</span><span class="p">)</span>

<span class="n">sp_cb</span> <span class="o">=</span> <span class="n">SparsifyCallback</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="s1">&#39;global&#39;</span><span class="p">,</span> <span class="n">large_final</span><span class="p">,</span> <span class="n">iterative</span><span class="p">,</span> <span class="n">start_epoch</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">lth</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rewind_epoch</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">cbs</span><span class="o">=</span><span class="n">sp_cb</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Pruning of weight until a sparsity of 50%
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.529935</td>
      <td>1.430763</td>
      <td>0.522548</td>
      <td>00:11</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.268891</td>
      <td>1.251196</td>
      <td>0.603822</td>
      <td>00:11</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.141558</td>
      <td>1.176961</td>
      <td>0.626497</td>
      <td>00:11</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.013069</td>
      <td>1.312681</td>
      <td>0.607134</td>
      <td>00:11</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.933651</td>
      <td>0.914163</td>
      <td>0.695796</td>
      <td>00:11</td>
    </tr>
    <tr>
      <td>5</td>
      <td>1.183302</td>
      <td>1.339694</td>
      <td>0.553121</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>6</td>
      <td>1.027278</td>
      <td>1.148169</td>
      <td>0.634904</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.919856</td>
      <td>1.031522</td>
      <td>0.672866</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.890848</td>
      <td>0.910739</td>
      <td>0.713885</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.824205</td>
      <td>0.932853</td>
      <td>0.697580</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>10</td>
      <td>1.054473</td>
      <td>1.329592</td>
      <td>0.585987</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.947696</td>
      <td>1.136064</td>
      <td>0.637452</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.852863</td>
      <td>0.820551</td>
      <td>0.731210</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.794559</td>
      <td>1.009437</td>
      <td>0.673631</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.775261</td>
      <td>0.844786</td>
      <td>0.721529</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.933353</td>
      <td>1.198227</td>
      <td>0.640000</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.846583</td>
      <td>0.898716</td>
      <td>0.715669</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.789335</td>
      <td>0.781211</td>
      <td>0.741656</td>
      <td>00:20</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.745516</td>
      <td>1.174927</td>
      <td>0.637962</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.705972</td>
      <td>0.786245</td>
      <td>0.751847</td>
      <td>00:20</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Sparsity at the end of epoch 0: 0.00%
Saving Weights at epoch 1
Sparsity at the end of epoch 1: 0.00%
Sparsity at the end of epoch 2: 0.00%
Sparsity at the end of epoch 3: 0.00%
Sparsity at the end of epoch 4: 0.00%
Resetting Weights to their epoch 1 values
Sparsity at the end of epoch 5: 16.67%
Sparsity at the end of epoch 6: 16.67%
Sparsity at the end of epoch 7: 16.67%
Sparsity at the end of epoch 8: 16.67%
Sparsity at the end of epoch 9: 16.67%
Resetting Weights to their epoch 1 values
Sparsity at the end of epoch 10: 33.33%
Sparsity at the end of epoch 11: 33.33%
Sparsity at the end of epoch 12: 33.33%
Sparsity at the end of epoch 13: 33.33%
Sparsity at the end of epoch 14: 33.33%
Resetting Weights to their epoch 1 values
Sparsity at the end of epoch 15: 50.00%
Sparsity at the end of epoch 16: 50.00%
Sparsity at the end of epoch 17: 50.00%
Sparsity at the end of epoch 18: 50.00%
Sparsity at the end of epoch 19: 50.00%
Final Sparsity: 50.00
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see here the benefits of rewinding, as the network has reached $75\%$ in $5$ epochs, which is better than plain LTH, but also <strong>way</strong> better than the original, dense model.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><p><strong>Remark:</strong> The current methods return the winning ticket after it has been trained, i.e. $W_T \odot m$ . If you would like to return the ticket re-initialized to its rewind epoch, i.e. the network $W_t \odot m$, just pass the argument <code>reset_end=True</code> to the callback.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It thus seem to be possible to train sparse networks, and that they even are able to overperform their dense counterpart ! I hope that this blog post gave you a better overview of what Lottery Tickets are and that you are now able to use this secret weapon in your projects. Go win yourself the initialization lottery ! ðŸŽ°</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p style="font-size: 15px"><i>If you notice any mistake or improvement that can be done, please contact me ! If you found that post useful, please consider citing it as:</i></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">

<pre><code>@article{hubens2020fasterai,
  title   = "Winning the Lottery with fastai",
  author  = "Hubens, Nathan",
  journal = "nathanhubens.github.io",
  year    = "2022",
  url     = "https://nathanhubens.github.io/posts/deep%20learning/2022/02/16/Lottery.html"
}</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="References"><strong>References</strong><a class="anchor-link" href="#References"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>{{'<a href="https://arxiv.org/pdf/2102.00554.pdf">Torsten Hoefler et al. Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks. JMLR, 2021</a>' | fndetail: 1}}</li>
<li>{{'<a href="https://arxiv.org/pdf/1803.03635.pdf">Jonathan Frankle and Michael Carbin. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.ICLR, 2019</a>' | fndetail: 2}}</li>
<li>{{'<a href="https://arxiv.org/pdf/1912.05671.pdf">Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. 2020. Linear Mode Connectivity and the Lottery Ticket Hypothesis. ICLR, 2020</a>' | fndetail: 3}}</li>
</ul>

</div>
</div>
</div>
</div>
 

