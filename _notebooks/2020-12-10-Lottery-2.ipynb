{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winning the Lottery with fastai\n",
    "> How to find winning tickets in your neural network\n",
    "\n",
    "- toc: true\n",
    "- badges: false\n",
    "- categories: [Deep Learning]\n",
    "- comments: true\n",
    "- image: images/pruning.png\n",
    "- hide: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lottery Ticket Hypothesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lottery Ticket Hypothesis is a fascinating characteristic of neural networks that has been found by Frankle and Carbin in 2019. The hypothesis is the following: in a neural network, there exists a subnetwork that can be trained to a comparable accuracy and in a comparable training time than the whole network. The only condition is that the subnetwork starts from the same initial condition than when it was part of the whole network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, this subnetwork, called \"winning ticket\", can be found by using pruning on the network, removing useless connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps to isolate this winning ticket are: \n",
    "1. Get a freshly initialized network\n",
    "2. Train it to convergence\n",
    "3. Prune the smallest weights, i.e. the weights that possess the lowest $l_1$-norm\n",
    "4. Reinitialize the remaining weights to their original value, i.e. their value at step 1)\n",
    "5. Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](images/LTH/test2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using fasterai, we already know how to prune a network. The only change here is that we have to keep track of initialization since we want to start from the initial conditions each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original paper, the idea was to iteratively prune the network, resetting the remaining weights to their initial value after each pruning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision.all import *\n",
    "from fasterai.sparse.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sparsifier():\n",
    "\n",
    "    def __init__(self, model, granularity, method, criteria, layer_type=nn.Conv2d):\n",
    "        store_attr()\n",
    "        self._save_weights() # Save the original weights\n",
    "\n",
    "    def prune_layer(self, m, sparsity, round_to=None):\n",
    "        weight = self.criteria(m, self.granularity)\n",
    "        mask = self._compute_mask(weight, sparsity, round_to)\n",
    "        m.register_buffer(\"_mask\", mask) # Put the mask into a buffer\n",
    "        self._apply(m)\n",
    "\n",
    "    def prune_model(self, sparsity, round_to=None):\n",
    "        self.threshold=None\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, self.layer_type): self.prune_layer(m, sparsity, round_to)\n",
    "\n",
    "    def _apply(self, m):\n",
    "        mask = getattr(m, \"_mask\", None)\n",
    "        if mask is not None: m.weight.data.mul_(mask)\n",
    "        if self.granularity == 'filter' and m.bias is not None:\n",
    "            if mask is not None: m.bias.data.mul_(mask.squeeze()) # We want to prune the bias when pruning filters\n",
    "\n",
    "    def _mask_grad(self):\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, self.layer_type) and hasattr(m, '_mask'):\n",
    "                mask = getattr(m, \"_mask\")\n",
    "                if m.weight.grad is not None: m.weight.grad.mul_(mask)\n",
    "                if self.granularity == 'filter' and m.bias is not None:\n",
    "                    if m.bias.grad is not None: m.bias.grad.mul_(mask.squeeze())\n",
    "\n",
    "    def _reset_weights(self): # Reset non-pruned weights\n",
    "        for m in self.model.modules():\n",
    "            if hasattr(m, 'weight'):\n",
    "                init_weights = getattr(m, \"_init_weights\", m.weight)\n",
    "                init_biases = getattr(m, \"_init_biases\", m.bias)\n",
    "                with torch.no_grad():\n",
    "                    if m.weight is not None: m.weight.copy_(init_weights)\n",
    "                    if m.bias is not None: m.bias.copy_(init_biases)\n",
    "                self._apply(m)\n",
    "            if isinstance(m, nn.modules.batchnorm._BatchNorm): m.reset_parameters()\n",
    "\n",
    "    def _save_weights(self):\n",
    "        for m in self.model.modules():\n",
    "            if hasattr(m, 'weight'):\n",
    "                m.register_buffer(\"_init_weights\", m.weight.clone())\n",
    "                b = getattr(m, 'bias', None)\n",
    "                if b is not None: m.register_buffer(\"_init_biases\", b.clone())\n",
    "\n",
    "    def _clean_buffers(self):\n",
    "        for m in self.model.modules():\n",
    "            if hasattr(m, 'weight'):\n",
    "                if hasattr(m, '_mask'): del m._buffers[\"_mask\"]\n",
    "                if hasattr(m, '_init_weights'): del m._buffers[\"_init_weights\"]\n",
    "                if hasattr(m, '_init_biases'): del m._buffers[\"_init_biases\"]\n",
    "\n",
    "    def _compute_threshold(self, weight, sparsity):\n",
    "        if self.method == 'global':\n",
    "            global_weight = torch.cat([self.criteria(m, self.granularity).view(-1) for m in self.model.modules() if isinstance(m, self.layer_type)])\n",
    "            if self.threshold is None: self.threshold = torch.quantile(global_weight, sparsity/100) # Compute the threshold globally (only once per model pruning)\n",
    "            return self.threshold\n",
    "        elif self.method == 'local':\n",
    "            return torch.quantile(weight.view(-1), sparsity/100) # Compute the threshold locally\n",
    "        else: raise NameError('Invalid Method')\n",
    "\n",
    "    def _rounded_sparsity(self, n_to_prune, round_to):\n",
    "        return max(round_to*torch.ceil(n_to_prune/round_to), round_to)\n",
    "\n",
    "    def _compute_mask(self, weight, sparsity, round_to):\n",
    "        threshold = self._compute_threshold(weight, sparsity)\n",
    "        if round_to:\n",
    "            n_to_keep = sum(weight.ge(threshold)).squeeze()\n",
    "            threshold = torch.topk(weight.squeeze(), int(self._rounded_sparsity(n_to_keep, round_to)))[0].min()\n",
    "        if threshold > weight.max(): threshold = weight.max() # Make sure we don't remove every weight of a given layer\n",
    "        return weight.ge(threshold).to(dtype=weight.dtype)\n",
    "\n",
    "    def print_sparsity(self):\n",
    "        for k,m in enumerate(self.model.modules()):\n",
    "            if isinstance(m, self.layer_type):\n",
    "                print(f\"Sparsity in {m.__class__.__name__} {k}: {100. * float(torch.sum(m.weight == 0))/ float(m.weight.nelement()):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparsifyCallback(Callback):\n",
    "\n",
    "    def __init__(self, end_sparsity, granularity, method, criteria, sched_func, start_sparsity=0, start_epoch=0, end_epoch=None, lth=False, rewind_epoch=0, reset_end=False, model=None, round_to=None, layer_type=nn.Conv2d):\n",
    "        store_attr()\n",
    "        self.current_sparsity, self.previous_sparsity = 0, 0\n",
    "\n",
    "        assert self.start_epoch>=self.rewind_epoch, 'You must rewind to an epoch before the start of the pruning process'\n",
    "\n",
    "    def before_fit(self):\n",
    "        print(f'Pruning of {self.granularity} until a sparsity of {self.end_sparsity}%')\n",
    "        self.end_epoch = self.n_epoch if self.end_epoch is None else self.end_epoch\n",
    "        assert self.end_epoch <= self.n_epoch, 'Your end_epoch must be smaller than total number of epoch'\n",
    "\n",
    "        model = self.learn.model if self.model is None else self.model # Pass a model if you don't want the whole model to be pruned\n",
    "        self.sparsifier = Sparsifier(model, self.granularity, self.method, self.criteria, self.layer_type)\n",
    "        self.n_batches = math.floor(len(self.learn.dls.dataset)/self.learn.dls.bs)\n",
    "        self.total_iters = self.end_epoch * self.n_batches\n",
    "        self.start_iter = self.start_epoch * self.n_batches\n",
    "\n",
    "    def before_epoch(self):\n",
    "        if self.epoch == self.rewind_epoch:\n",
    "            print(f'Saving Weights at epoch {self.epoch}')\n",
    "            self.sparsifier._save_weights()\n",
    "\n",
    "    def before_batch(self):\n",
    "        if self.epoch>=self.start_epoch:\n",
    "            if self.epoch < self.end_epoch: self._set_sparsity()\n",
    "            self.sparsifier.prune_model(self.current_sparsity, self.round_to)\n",
    "\n",
    "            if self.lth and self.current_sparsity!=self.previous_sparsity: # If sparsity has changed, the network has been pruned\n",
    "                    print(f'Resetting Weights to their epoch {self.rewind_epoch} values')\n",
    "                    self.sparsifier._reset_weights()\n",
    "                    #self.sparsifier.model = resnet18(num_classes=10)\n",
    "\n",
    "            self.previous_sparsity = self.current_sparsity\n",
    "\n",
    "    def before_step(self):\n",
    "        if self.epoch>=self.start_epoch:\n",
    "            self.sparsifier._mask_grad()\n",
    "\n",
    "    def after_epoch(self):\n",
    "        print(f'Sparsity at the end of epoch {self.epoch}: {self.current_sparsity:.2f}%')\n",
    "\n",
    "    def after_fit(self):\n",
    "        print(f'Final Sparsity: {self.current_sparsity:.2f}')\n",
    "        if self.reset_end:\n",
    "            self.sparsifier._reset_weights()\n",
    "        self.sparsifier._clean_buffers() # Remove buffers at the end of training\n",
    "        #self.sparsifier.print_sparsity()\n",
    "\n",
    "    def _set_sparsity(self):\n",
    "        self.current_sparsity = self.sched_func(start=self.start_sparsity, end=self.end_sparsity, pos=(self.train_iter-self.start_iter)/(self.total_iters-self.start_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def get_dls(size, pct_noise, bs, device):\n",
    "    assert pct_noise in [0,5,50], '`pct_noise` must be 0,5 or 50.'\n",
    "    path = URLs.IMAGENETTE_320\n",
    "    source = untar_data(path)\n",
    "    blocks=(ImageBlock, CategoryBlock)\n",
    "    tfms = [RandomResizedCrop(size, min_scale=0.35), FlipItem(0.5)]\n",
    "    batch_tfms = [Normalize.from_stats(*imagenet_stats)]\n",
    "    \n",
    "    csv_file = 'noisy_imagenette.csv'\n",
    "    inp = pd.read_csv(source/csv_file)\n",
    "    dblock = DataBlock(blocks=blocks,\n",
    "               splitter=ColSplitter(),\n",
    "               get_x=ColReader('path', pref=source), \n",
    "               get_y=ColReader(f'noisy_labels_{pct_noise}'),\n",
    "               item_tfms=tfms,\n",
    "               batch_tfms=batch_tfms)\n",
    "    \n",
    "    return dblock.dataloaders(inp, path=source, bs=bs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def count_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'Total parameters : {num_params:,}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def print_sparsity(model):\n",
    "    for k,m in enumerate(model.modules()):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            print(f\"Sparsity in {m.__class__.__name__} {k}: {100. * float(torch.sum(m.weight == 0))/ float(m.weight.nelement()):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "dls = get_dls(128, 0, 64, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first get our baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, resnet18(num_classes=10), metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = deepcopy(learn.model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.536754</td>\n",
       "      <td>1.709699</td>\n",
       "      <td>0.481529</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.254531</td>\n",
       "      <td>1.314451</td>\n",
       "      <td>0.578089</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.116412</td>\n",
       "      <td>1.168404</td>\n",
       "      <td>0.634904</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.023481</td>\n",
       "      <td>1.156428</td>\n",
       "      <td>0.633376</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.946494</td>\n",
       "      <td>0.998459</td>\n",
       "      <td>0.677962</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = Learner(dls, resnet18(num_classes=10), metrics=accuracy)\n",
    "learn.model.load_state_dict(initial_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fasterai, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_cb = SparsifyCallback(50, 'weight', 'global', large_final, iterative, start_epoch=5, lth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning of weight until a sparsity of 50%\n",
      "Saving Weights at epoch 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.541520</td>\n",
       "      <td>1.568734</td>\n",
       "      <td>0.501911</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.258532</td>\n",
       "      <td>1.628220</td>\n",
       "      <td>0.508790</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.111838</td>\n",
       "      <td>1.292680</td>\n",
       "      <td>0.596688</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.024304</td>\n",
       "      <td>1.385538</td>\n",
       "      <td>0.581146</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.930883</td>\n",
       "      <td>1.041547</td>\n",
       "      <td>0.672102</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.330930</td>\n",
       "      <td>1.395270</td>\n",
       "      <td>0.520510</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.141437</td>\n",
       "      <td>1.135004</td>\n",
       "      <td>0.620637</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.040761</td>\n",
       "      <td>1.267395</td>\n",
       "      <td>0.581656</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.952175</td>\n",
       "      <td>1.272328</td>\n",
       "      <td>0.594650</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.909871</td>\n",
       "      <td>1.207141</td>\n",
       "      <td>0.629554</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.235558</td>\n",
       "      <td>1.197264</td>\n",
       "      <td>0.598217</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.042131</td>\n",
       "      <td>1.067109</td>\n",
       "      <td>0.658854</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.927392</td>\n",
       "      <td>0.977499</td>\n",
       "      <td>0.673376</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.888816</td>\n",
       "      <td>0.916399</td>\n",
       "      <td>0.699873</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.800480</td>\n",
       "      <td>0.774320</td>\n",
       "      <td>0.743439</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.052142</td>\n",
       "      <td>1.027188</td>\n",
       "      <td>0.665223</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.921996</td>\n",
       "      <td>0.945266</td>\n",
       "      <td>0.694268</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.831712</td>\n",
       "      <td>0.868593</td>\n",
       "      <td>0.717452</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.812539</td>\n",
       "      <td>1.016729</td>\n",
       "      <td>0.673376</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.764737</td>\n",
       "      <td>0.859072</td>\n",
       "      <td>0.725860</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity at the end of epoch 0: 0.00%\n",
      "Sparsity at the end of epoch 1: 0.00%\n",
      "Sparsity at the end of epoch 2: 0.00%\n",
      "Sparsity at the end of epoch 3: 0.00%\n",
      "Sparsity at the end of epoch 4: 0.00%\n",
      "Resetting Weights to their epoch 0 values\n",
      "Sparsity at the end of epoch 5: 16.67%\n",
      "Sparsity at the end of epoch 6: 16.67%\n",
      "Sparsity at the end of epoch 7: 16.67%\n",
      "Sparsity at the end of epoch 8: 16.67%\n",
      "Sparsity at the end of epoch 9: 16.67%\n",
      "Resetting Weights to their epoch 0 values\n",
      "Sparsity at the end of epoch 10: 33.33%\n",
      "Sparsity at the end of epoch 11: 33.33%\n",
      "Sparsity at the end of epoch 12: 33.33%\n",
      "Sparsity at the end of epoch 13: 33.33%\n",
      "Sparsity at the end of epoch 14: 33.33%\n",
      "Resetting Weights to their epoch 0 values\n",
      "Sparsity at the end of epoch 15: 50.00%\n",
      "Sparsity at the end of epoch 16: 50.00%\n",
      "Sparsity at the end of epoch 17: 50.00%\n",
      "Sparsity at the end of epoch 18: 50.00%\n",
      "Sparsity at the end of epoch 19: 50.00%\n",
      "Final Sparsity: 50.00\n"
     ]
    }
   ],
   "source": [
    "learn.fit(20, cbs=sp_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = Learner(dls, resnet18(num_classes=10), metrics=accuracy)\n",
    "learn.model.load_state_dict(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_cb = SparsifyCallback(50, 'weight', 'global', large_final, iterative, start_epoch=5, lth=True, rewind_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning of weight until a sparsity of 50%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.529935</td>\n",
       "      <td>1.430763</td>\n",
       "      <td>0.522548</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.268891</td>\n",
       "      <td>1.251196</td>\n",
       "      <td>0.603822</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.141558</td>\n",
       "      <td>1.176961</td>\n",
       "      <td>0.626497</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.013069</td>\n",
       "      <td>1.312681</td>\n",
       "      <td>0.607134</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.933651</td>\n",
       "      <td>0.914163</td>\n",
       "      <td>0.695796</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.183302</td>\n",
       "      <td>1.339694</td>\n",
       "      <td>0.553121</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.027278</td>\n",
       "      <td>1.148169</td>\n",
       "      <td>0.634904</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.919856</td>\n",
       "      <td>1.031522</td>\n",
       "      <td>0.672866</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.890848</td>\n",
       "      <td>0.910739</td>\n",
       "      <td>0.713885</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.824205</td>\n",
       "      <td>0.932853</td>\n",
       "      <td>0.697580</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.054473</td>\n",
       "      <td>1.329592</td>\n",
       "      <td>0.585987</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.947696</td>\n",
       "      <td>1.136064</td>\n",
       "      <td>0.637452</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.852863</td>\n",
       "      <td>0.820551</td>\n",
       "      <td>0.731210</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.794559</td>\n",
       "      <td>1.009437</td>\n",
       "      <td>0.673631</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.775261</td>\n",
       "      <td>0.844786</td>\n",
       "      <td>0.721529</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.933353</td>\n",
       "      <td>1.198227</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.846583</td>\n",
       "      <td>0.898716</td>\n",
       "      <td>0.715669</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.789335</td>\n",
       "      <td>0.781211</td>\n",
       "      <td>0.741656</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.745516</td>\n",
       "      <td>1.174927</td>\n",
       "      <td>0.637962</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.705972</td>\n",
       "      <td>0.786245</td>\n",
       "      <td>0.751847</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity at the end of epoch 0: 0.00%\n",
      "Saving Weights at epoch 1\n",
      "Sparsity at the end of epoch 1: 0.00%\n",
      "Sparsity at the end of epoch 2: 0.00%\n",
      "Sparsity at the end of epoch 3: 0.00%\n",
      "Sparsity at the end of epoch 4: 0.00%\n",
      "Resetting Weights to their epoch 1 values\n",
      "Sparsity at the end of epoch 5: 16.67%\n",
      "Sparsity at the end of epoch 6: 16.67%\n",
      "Sparsity at the end of epoch 7: 16.67%\n",
      "Sparsity at the end of epoch 8: 16.67%\n",
      "Sparsity at the end of epoch 9: 16.67%\n",
      "Resetting Weights to their epoch 1 values\n",
      "Sparsity at the end of epoch 10: 33.33%\n",
      "Sparsity at the end of epoch 11: 33.33%\n",
      "Sparsity at the end of epoch 12: 33.33%\n",
      "Sparsity at the end of epoch 13: 33.33%\n",
      "Sparsity at the end of epoch 14: 33.33%\n",
      "Resetting Weights to their epoch 1 values\n",
      "Sparsity at the end of epoch 15: 50.00%\n",
      "Sparsity at the end of epoch 16: 50.00%\n",
      "Sparsity at the end of epoch 17: 50.00%\n",
      "Sparsity at the end of epoch 18: 50.00%\n",
      "Sparsity at the end of epoch 19: 50.00%\n",
      "Final Sparsity: 50.00\n"
     ]
    }
   ],
   "source": [
    "learn.fit(20, cbs=sp_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in Conv2d 1: 41.28%\n",
      "Sparsity in Conv2d 7: 25.14%\n",
      "Sparsity in Conv2d 10: 25.66%\n",
      "Sparsity in Conv2d 13: 25.76%\n",
      "Sparsity in Conv2d 16: 25.46%\n",
      "Sparsity in Conv2d 20: 33.97%\n",
      "Sparsity in Conv2d 23: 34.10%\n",
      "Sparsity in Conv2d 26: 12.82%\n",
      "Sparsity in Conv2d 29: 34.61%\n",
      "Sparsity in Conv2d 32: 33.99%\n",
      "Sparsity in Conv2d 36: 43.95%\n",
      "Sparsity in Conv2d 39: 44.27%\n",
      "Sparsity in Conv2d 42: 18.27%\n",
      "Sparsity in Conv2d 45: 43.93%\n",
      "Sparsity in Conv2d 48: 43.33%\n",
      "Sparsity in Conv2d 52: 54.12%\n",
      "Sparsity in Conv2d 55: 54.15%\n",
      "Sparsity in Conv2d 58: 24.68%\n",
      "Sparsity in Conv2d 61: 54.24%\n",
      "Sparsity in Conv2d 64: 51.93%\n"
     ]
    }
   ],
   "source": [
    "print_sparsity(learn.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That's all! Thank you for reading, I hope that you'll like FasterAI. I do not claim that it is perfect, you'll probably find a lot of bugs. If you do, just please tell me, so I can try to solve them ðŸ˜Œ **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 15px\"><i>If you notice any mistake or improvement that can be done, please contact me ! If you found that post useful, please consider citing it as:</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "@article{hubens2020fasterai,\n",
    "  title   = \"Winning the Lottery with fastai\",\n",
    "  author  = \"Hubens, Nathan\",\n",
    "  journal = \"nathanhubens.github.io\",\n",
    "  year    = \"2020\",\n",
    "  url     = \"https://nathanhubens.github.io/posts/deep%20learning/2020/08/17/FasterAI.html\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- {{'[Cristian BuciluÇŽ, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, 2006](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf)' | fndetail: 1}}\n",
    "- {{'[Qizhe Xie, Minh-Thang Luong, Eduard Hovy, Quoc V. Le: Self-training with Noisy Student improves ImageNet classification. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020](https://arxiv.org/abs/1911.04252)' | fndetail: 2}}\n",
    "- {{'[H. Li, \"Exploring knowledge distillation of Deep neural nets for efficient hardware solutions,\" CS230 Report, 2018](http://cs230.stanford.edu/files_winter_2018/projects/6940224.pdf)' | fndetail: 3}}\n",
    "- {{'[Zhu, M. & Gupta, S. (2017). To prune, or not to prune: exploring the efficacy of pruning for model compression. ICLR, 2018 ](https://openreview.net/pdf?id=Sy1iIDkPM)' | fndetail: 4}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
