{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winning the Lottery with fastai\n",
    "> How to find winning tickets in your neural network\n",
    "\n",
    "- toc: true\n",
    "- badges: false\n",
    "- categories: [Deep Learning]\n",
    "- comments: true\n",
    "- image: images/LTH.png\n",
    "- hide: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating sparse neural networks is a very hot topic at the moment. It is believed to make them **smaller**, **faster** and with **better generalization** capabilities{% fn 1 %}. For a long time however, it was believed that sparse networks were difficult to train. The traditional way of getting them was therefore to first train a dense network to convergence, then prune it to make it sparse, eventually fine-tuning it a tiny bit more to recover performance. However, some recent research has shown that not only it was possible to train sparse networks, but also that they may outperform their more-parameterized, dense, counterpart. The paper that initiated this trend talks about \"lottery tickets\", that may be hidden in neural networks{% fn 2 %}. In this blog post, we are going to explain what they are and how we can find them, with the help of [fastai](https://docs.fast.ai), and more particularly [fasterai](https://nathanhubens.github.io/fasterai/), a library to create smaller and faster neural networks that we created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lottery Ticket Hypothesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first introduce what The Lottery Ticket Hypothesis is for those who may have never heard about it. It is a fascinating characteristic of neural networks that has been discovered by Frankle and Carbin in 2019{% fn 2 %}. The gist of this hypothesis can be phrased as the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In a neural network, there exists a subnetwork that can be trained to at least the same accuracy and in at most the same training time as the whole network. The only condition being that both this sub- and the complete networks start from the same initial conditions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subnetwork, called the \"winning ticket\" (as it is believed to have won at the initialization lottery), can be found by using pruning on the network, removing useless connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps to unveil this winning ticket are: \n",
    "1. Get a freshly initialized network, possessing a set of weights $W_0$\n",
    "2. Train it for a certain amount $T$ of iterations, giving us the network with weights $W_T$\n",
    "3. Prune a portion of the smallest weights, i.e. the weights that possess the lowest $l_1$-norm, giving us the network with weights $W_T \\odot m$, with $m$ being a binary mask constituted of $0$ for weights we want to remove and $1$ for those we want to keep.\n",
    "4. Reinitialize the remaining weights to their original value, i.e. their value at step 1), giving us the network with weights $W_0 \\odot m$.\n",
    "5. Stop if target sparsity is reached or go back to step 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](images/LTH/test2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision.all import *\n",
    "from fasterai.sparse.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will conduct this tutorial by using a ResNet-18 architecture, trained on [Imagenette](https://github.com/fastai/imagenette), a subpart of Imagenet using only 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def get_dls(size, pct_noise, bs, device):\n",
    "    assert pct_noise in [0,5,50], '`pct_noise` must be 0,5 or 50.'\n",
    "    path = URLs.IMAGENETTE_320\n",
    "    source = untar_data(path)\n",
    "    blocks=(ImageBlock, CategoryBlock)\n",
    "    tfms = [RandomResizedCrop(size, min_scale=0.35), FlipItem(0.5)]\n",
    "    batch_tfms = [Normalize.from_stats(*imagenet_stats)]\n",
    "    \n",
    "    csv_file = 'noisy_imagenette.csv'\n",
    "    inp = pd.read_csv(source/csv_file)\n",
    "    dblock = DataBlock(blocks=blocks,\n",
    "               splitter=ColSplitter(),\n",
    "               get_x=ColReader('path', pref=source), \n",
    "               get_y=ColReader(f'noisy_labels_{pct_noise}'),\n",
    "               item_tfms=tfms,\n",
    "               batch_tfms=batch_tfms)\n",
    "    \n",
    "    return dblock.dataloaders(inp, path=source, bs=bs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "dls = get_dls(128, 0, 64, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first want a baseline of the complete model that we can then compare to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, resnet18(num_classes=10), metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the weights of this model, so that we can be sure to start from the exact same network in our further experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = deepcopy(learn.model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is our baseline, it will not be pruned. Thus, this model corresponds to $W_T$, with $T$ chosen to be $5$ epochs. So let's train it and report the final accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.536754</td>\n",
       "      <td>1.709699</td>\n",
       "      <td>0.481529</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.254531</td>\n",
       "      <td>1.314451</td>\n",
       "      <td>0.578089</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.116412</td>\n",
       "      <td>1.168404</td>\n",
       "      <td>0.634904</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.023481</td>\n",
       "      <td>1.156428</td>\n",
       "      <td>0.633376</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.946494</td>\n",
       "      <td>0.998459</td>\n",
       "      <td>0.677962</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, our baseline network is $68\\%$ accurate at discriminating between the 10 classes of our validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Can we please find winning tickets now ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already shown in a [previous blog](https://nathanhubens.github.io/posts/deep%20learning/2020/08/17/FasterAI.html#Sparsifying) post how to prune a network with fasterai. As a quick reminder, this can be done by using the `SparsifyCallback` callback during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only things to specify in the callback are:\n",
    "- `end_sparsity`, the target final level of sparsity in the network\n",
    "- `granularity`, the shape of parameters to remove, e.g. `weight` or `filter`\n",
    "- `method`, either prune the weights in each layer separately (`local`) or in the whole network (`global`)\n",
    "- `criteria`, i.e. how to score the importance of parameters to remove\n",
    "- `schedule`, i.e. when pruning is applied during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original paper, authors discover tickets using an Iterative Magnitude Pruning (IMP), meaning that the pruning is performed *iteratively*, with a *criteria* based on magnitude, i.e. the $l_1$-norm of weights. Authors also specify that they remove individual *weights*, comparing them across the network *globally*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily for us, all of these were already available in fasterai! We now know most of the parameters of our callback:\n",
    "`SparsifyCallback(end_sparsity, granularity='weight', method='global', criteria=large_final, schedule=iterative)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set then ! Well almost... If you remember correctly the 5 steps presented earlier, we need to keep track of the set of weights $W_0$, at initialization. We also need to reset our weights to their initial value after each pruning step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fasterai this can be done by:\n",
    "- passing the `lth` argument to `True`. Behind the hood, **fasterai** will save the initial weights of the model and reset them after each pruning step\n",
    "- Optionnally setting a `start_epoch`, which affects at which epoch the pruning process will start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recreate the exact same model as the one we used for baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = Learner(dls, resnet18(num_classes=10), metrics=accuracy)\n",
    "learn.model.load_state_dict(initial_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fasterai, the `iterative` schedule has 3 steps by default, which can easily be changed but we'll stick with it for our experiments.\n",
    "\n",
    "We'll thus have 3 rounds of pruning, and that our network will therefor be reset 3 times. As we want the network to be trained for $T=5$ epochs at each round, this means that the total epochs over which pruning will occur is $3 \\times 5 = 15$ epochs.\n",
    "\n",
    "**But** before performing any round of pruning, there is first a pretraining phase of $T$ epochs. The total number of epochs is then $20$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train this bad boy and see what happens !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning of weight until a sparsity of 50%\n",
      "Saving Weights at epoch 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.541520</td>\n",
       "      <td>1.568734</td>\n",
       "      <td>0.501911</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.258532</td>\n",
       "      <td>1.628220</td>\n",
       "      <td>0.508790</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.111838</td>\n",
       "      <td>1.292680</td>\n",
       "      <td>0.596688</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.024304</td>\n",
       "      <td>1.385538</td>\n",
       "      <td>0.581146</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.930883</td>\n",
       "      <td>1.041547</td>\n",
       "      <td>0.672102</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.330930</td>\n",
       "      <td>1.395270</td>\n",
       "      <td>0.520510</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.141437</td>\n",
       "      <td>1.135004</td>\n",
       "      <td>0.620637</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.040761</td>\n",
       "      <td>1.267395</td>\n",
       "      <td>0.581656</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.952175</td>\n",
       "      <td>1.272328</td>\n",
       "      <td>0.594650</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.909871</td>\n",
       "      <td>1.207141</td>\n",
       "      <td>0.629554</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.235558</td>\n",
       "      <td>1.197264</td>\n",
       "      <td>0.598217</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.042131</td>\n",
       "      <td>1.067109</td>\n",
       "      <td>0.658854</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.927392</td>\n",
       "      <td>0.977499</td>\n",
       "      <td>0.673376</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.888816</td>\n",
       "      <td>0.916399</td>\n",
       "      <td>0.699873</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.800480</td>\n",
       "      <td>0.774320</td>\n",
       "      <td>0.743439</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.052142</td>\n",
       "      <td>1.027188</td>\n",
       "      <td>0.665223</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.921996</td>\n",
       "      <td>0.945266</td>\n",
       "      <td>0.694268</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.831712</td>\n",
       "      <td>0.868593</td>\n",
       "      <td>0.717452</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.812539</td>\n",
       "      <td>1.016729</td>\n",
       "      <td>0.673376</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.764737</td>\n",
       "      <td>0.859072</td>\n",
       "      <td>0.725860</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity at the end of epoch 0: 0.00%\n",
      "Sparsity at the end of epoch 1: 0.00%\n",
      "Sparsity at the end of epoch 2: 0.00%\n",
      "Sparsity at the end of epoch 3: 0.00%\n",
      "Sparsity at the end of epoch 4: 0.00%\n",
      "Resetting Weights to their epoch 0 values\n",
      "Sparsity at the end of epoch 5: 16.67%\n",
      "Sparsity at the end of epoch 6: 16.67%\n",
      "Sparsity at the end of epoch 7: 16.67%\n",
      "Sparsity at the end of epoch 8: 16.67%\n",
      "Sparsity at the end of epoch 9: 16.67%\n",
      "Resetting Weights to their epoch 0 values\n",
      "Sparsity at the end of epoch 10: 33.33%\n",
      "Sparsity at the end of epoch 11: 33.33%\n",
      "Sparsity at the end of epoch 12: 33.33%\n",
      "Sparsity at the end of epoch 13: 33.33%\n",
      "Sparsity at the end of epoch 14: 33.33%\n",
      "Resetting Weights to their epoch 0 values\n",
      "Sparsity at the end of epoch 15: 50.00%\n",
      "Sparsity at the end of epoch 16: 50.00%\n",
      "Sparsity at the end of epoch 17: 50.00%\n",
      "Sparsity at the end of epoch 18: 50.00%\n",
      "Sparsity at the end of epoch 19: 50.00%\n",
      "Final Sparsity: 50.00\n"
     ]
    }
   ],
   "source": [
    "sp_cb = SparsifyCallback(50, 'weight', 'global', large_final, iterative, start_epoch=5, lth=True)\n",
    "learn.fit(20, cbs=sp_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the verbose below training results, the weights are reset to their original value every 5 epochs. This can also be observed when looking at the accuracy, which drops after each pruning round."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each round, the sparsity level is increased, meaning that the binary mask $m$ in $W_T \\odot m$ has more and more zeroes as the training goes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last round, performed at a constant sparsity level of $50\\%$, is able to reach $72\\%$ of accuracy in 5 epochs, which is better than our baseline !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lottery Ticket Hypothesis with Rewinding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, authors noticed that this IMP procedure may fail on deeper networks{% fn 3 %}, they thus propose to weaken the original Lottery Ticket Hypothesis, making the network to be reset to weights early in training instead of at initialization, i.e. our step 4) now resets the weights to $W_t \\odot m$ with $t<T$. Such a subnetwork is no longer called a \"winning\" ticket, but a \"matching\" ticket. In this case, the regular LTH is just the particular case of $t=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fasterai, this can be done by changing the `rewind_epoch` value to the epoch you want your weights to be reset to, everything else stays the same. Let's try this !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning of weight until a sparsity of 50%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.529935</td>\n",
       "      <td>1.430763</td>\n",
       "      <td>0.522548</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.268891</td>\n",
       "      <td>1.251196</td>\n",
       "      <td>0.603822</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.141558</td>\n",
       "      <td>1.176961</td>\n",
       "      <td>0.626497</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.013069</td>\n",
       "      <td>1.312681</td>\n",
       "      <td>0.607134</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.933651</td>\n",
       "      <td>0.914163</td>\n",
       "      <td>0.695796</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.183302</td>\n",
       "      <td>1.339694</td>\n",
       "      <td>0.553121</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.027278</td>\n",
       "      <td>1.148169</td>\n",
       "      <td>0.634904</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.919856</td>\n",
       "      <td>1.031522</td>\n",
       "      <td>0.672866</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.890848</td>\n",
       "      <td>0.910739</td>\n",
       "      <td>0.713885</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.824205</td>\n",
       "      <td>0.932853</td>\n",
       "      <td>0.697580</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.054473</td>\n",
       "      <td>1.329592</td>\n",
       "      <td>0.585987</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.947696</td>\n",
       "      <td>1.136064</td>\n",
       "      <td>0.637452</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.852863</td>\n",
       "      <td>0.820551</td>\n",
       "      <td>0.731210</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.794559</td>\n",
       "      <td>1.009437</td>\n",
       "      <td>0.673631</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.775261</td>\n",
       "      <td>0.844786</td>\n",
       "      <td>0.721529</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.933353</td>\n",
       "      <td>1.198227</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.846583</td>\n",
       "      <td>0.898716</td>\n",
       "      <td>0.715669</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.789335</td>\n",
       "      <td>0.781211</td>\n",
       "      <td>0.741656</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.745516</td>\n",
       "      <td>1.174927</td>\n",
       "      <td>0.637962</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.705972</td>\n",
       "      <td>0.786245</td>\n",
       "      <td>0.751847</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity at the end of epoch 0: 0.00%\n",
      "Saving Weights at epoch 1\n",
      "Sparsity at the end of epoch 1: 0.00%\n",
      "Sparsity at the end of epoch 2: 0.00%\n",
      "Sparsity at the end of epoch 3: 0.00%\n",
      "Sparsity at the end of epoch 4: 0.00%\n",
      "Resetting Weights to their epoch 1 values\n",
      "Sparsity at the end of epoch 5: 16.67%\n",
      "Sparsity at the end of epoch 6: 16.67%\n",
      "Sparsity at the end of epoch 7: 16.67%\n",
      "Sparsity at the end of epoch 8: 16.67%\n",
      "Sparsity at the end of epoch 9: 16.67%\n",
      "Resetting Weights to their epoch 1 values\n",
      "Sparsity at the end of epoch 10: 33.33%\n",
      "Sparsity at the end of epoch 11: 33.33%\n",
      "Sparsity at the end of epoch 12: 33.33%\n",
      "Sparsity at the end of epoch 13: 33.33%\n",
      "Sparsity at the end of epoch 14: 33.33%\n",
      "Resetting Weights to their epoch 1 values\n",
      "Sparsity at the end of epoch 15: 50.00%\n",
      "Sparsity at the end of epoch 16: 50.00%\n",
      "Sparsity at the end of epoch 17: 50.00%\n",
      "Sparsity at the end of epoch 18: 50.00%\n",
      "Sparsity at the end of epoch 19: 50.00%\n",
      "Final Sparsity: 50.00\n"
     ]
    }
   ],
   "source": [
    "learn = Learner(dls, resnet18(num_classes=10), metrics=accuracy)\n",
    "learn.model.load_state_dict(initial_weights)\n",
    "\n",
    "sp_cb = SparsifyCallback(50, 'weight', 'global', large_final, iterative, start_epoch=5, lth=True, rewind_epoch=1)\n",
    "learn.fit(20, cbs=sp_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here the benefits of rewinding, as the network has reached $75\\%$ in $5$ epochs, which is better than plain LTH, but also **way** better than the original, dense model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Remark:** The current methods return the winning ticket after it has been trained, i.e. $W_T \\odot m$ . If you would like to return the ticket re-initialized to its rewind epoch, i.e. the network $W_t \\odot m$, just pass the argument `reset_end=True` to the callback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It thus seem to be possible to train sparse networks, and that they even are able to overperform their dense counterpart ! I hope that this blog post gave you a better overview of what Lottery Tickets are and that you are now able to use this secret weapon in your projects. Go win yourself the initialization lottery ! ðŸŽ° "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 15px\"><i>If you notice any mistake or improvement that can be done, please contact me ! If you found that post useful, please consider citing it as:</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "@article{hubens2020fasterai,\n",
    "  title   = \"Winning the Lottery with fastai\",\n",
    "  author  = \"Hubens, Nathan\",\n",
    "  journal = \"nathanhubens.github.io\",\n",
    "  year    = \"2022\",\n",
    "  url     = \"https://nathanhubens.github.io/posts/deep%20learning/2022/02/16/Lottery.html\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- {{'[Torsten Hoefler et al. Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks. JMLR, 2021](https://arxiv.org/pdf/2102.00554.pdf)' | fndetail: 1}}\n",
    "- {{'[Jonathan Frankle and Michael Carbin. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.ICLR, 2019](https://arxiv.org/pdf/1803.03635.pdf)' | fndetail: 2}}\n",
    "- {{'[Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. 2020. Linear Mode Connectivity and the Lottery Ticket Hypothesis. ICLR, 2020](https://arxiv.org/pdf/1912.05671.pdf)' | fndetail: 3}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
