{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winning the Lottery with fastai\n",
    "> How to find winning tickets in your neural network\n",
    "\n",
    "- toc: true\n",
    "- badges: false\n",
    "- categories: [Deep Learning]\n",
    "- comments: true\n",
    "- image: images/pruning.png\n",
    "- hide: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lottery Ticket Hypothesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lottery Ticket Hypothesis is a fascinating characteristic of neural networks that has been found by Frankle and Carbin in 2019. The hypothesis is the following: in a neural network, there exists a subnetwork that can be trained to a comparable accuracy and in a comparable training time than the whole network. The only condition is that the subnetwork starts from the same initial condition than when it was part of the whole network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, this subnetwork, called \"winning ticket\", can be found by using pruning on the network, removing useless connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps to isolate this winning ticket are: \n",
    "1. Get a freshly initialized network\n",
    "2. Train it to convergence\n",
    "3. Prune the smallest weights, i.e. the weights that possess the lowest $l_1$-norm\n",
    "4. Reinitialize the remaining weights to their original value, i.e. their value at step 1)\n",
    "5. Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](images/LTH/test2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using fasterai, we already know how to prune a network. The only change here is that we have to keep track of initialization since we want to start from the initial conditions each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original paper, the idea was to iteratively prune the network, resetting the remaining weights to their initial value after each pruning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def get_data(size, bs):\n",
    "    path = untar_data(URLs.IMAGENETTE_160)\n",
    "\n",
    "    return (ImageList.from_folder(path).split_by_folder(valid='val')\n",
    "            .label_from_folder().transform(([flip_lr(p=0.5)], []), size=size)\n",
    "            .databunch(bs=bs)\n",
    "            .presize(size, scale=(0.35,1))\n",
    "            .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def count_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'Total parameters : {num_params:,}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def print_sparsity(model):\n",
    "    for k,m in enumerate(model.modules()):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            print(f\"Sparsity in {m.__class__.__name__} {k}: {100. * float(torch.sum(m.weight == 0))/ float(m.weight.nelement()):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "size, bs = 224, 16\n",
    "data = get_data(size, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first get our baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, models.resnet18(num_classes=10), metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.016354</td>\n",
       "      <td>1.778865</td>\n",
       "      <td>0.368917</td>\n",
       "      <td>01:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.777570</td>\n",
       "      <td>1.508860</td>\n",
       "      <td>0.523567</td>\n",
       "      <td>01:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.436139</td>\n",
       "      <td>1.421571</td>\n",
       "      <td>0.569172</td>\n",
       "      <td>01:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.275864</td>\n",
       "      <td>1.118840</td>\n",
       "      <td>0.630064</td>\n",
       "      <td>01:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.136620</td>\n",
       "      <td>0.994999</td>\n",
       "      <td>0.687898</td>\n",
       "      <td>01:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.970474</td>\n",
       "      <td>0.824344</td>\n",
       "      <td>0.739618</td>\n",
       "      <td>01:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.878756</td>\n",
       "      <td>0.764273</td>\n",
       "      <td>0.765605</td>\n",
       "      <td>01:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.817084</td>\n",
       "      <td>0.710727</td>\n",
       "      <td>0.781911</td>\n",
       "      <td>01:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.716041</td>\n",
       "      <td>0.625853</td>\n",
       "      <td>0.804841</td>\n",
       "      <td>01:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.668815</td>\n",
       "      <td>0.605727</td>\n",
       "      <td>0.810955</td>\n",
       "      <td>01:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(10, 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would be the performance of our model with regular pruning ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try the LTH. The first test will be using One-Shot, i.e. we will prune our network and reset the weights once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is thus to get our pruned model, then setting the parameter `reset_end` to `lth`, meaning that after the training, we will reset the remaining weights to their original initialization value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, models.resnet18(num_classes=10), metrics=[accuracy])\n",
    "sched_func = annealing_cos\n",
    "\n",
    "prune = SparsifyCallback(learn, sparsity=sp, granularity=granularity, method=method, criteria=criteria, sched_func=sched_func, reset='lth')\n",
    "learn.fit_one_cycle(int(epochs), 1e-3, callbacks=[prune])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can retrain the submodel, starting with their original initialization values, for the same amount of epochs and check if the performance is comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = SparsifyCallback(learn, sparsity=sp, granularity=granularity, method=method, criteria=criteria, sched_func=annealing_no)\n",
    "\n",
    "learn.fit_one_cycle(int(epochs), 1e-3, callbacks=[ft])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LTH can also be done iteratively, with each pruning iteration being followed by a weight reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, models.resnet18(num_classes=10), metrics=[accuracy])\n",
    "sched_func = iterative\n",
    "\n",
    "prune = SparsifyCallback(learn, sparsity=sp, granularity=granularity, method=method, criteria=criteria, sched_func=sched_func, reset='lth')\n",
    "learn.fit_one_cycle(int(epochs), 1e-3, callbacks=[prune])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, authors have suggested that one shouldn't necessarily reset the weights to their initial value, i.e their value at step 0 but at a further step. This can be done by changing the `rewind` value to the epoch you want your weights to be reset to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, models.resnet18(num_classes=10), metrics=[accuracy])\n",
    "sched_func = iterative\n",
    "\n",
    "prune = SparsifyCallback(learn, sparsity=sp, granularity=granularity, method=method, criteria=criteria, sched_func=sched_func, reset='lth', rewind=2)\n",
    "learn.fit_one_cycle(int(epochs), 1e-3, callbacks=[prune])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That's all! Thank you for reading, I hope that you'll like FasterAI. I do not claim that it is perfect, you'll probably find a lot of bugs. If you do, just please tell me, so I can try to solve them 😌 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 15px\"><i>If you notice any mistake or improvement that can be done, please contact me ! If you found that post useful, please consider citing it as:</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "@article{hubens2020fasterai,\n",
    "  title   = \"Winning the Lottery with fastai\",\n",
    "  author  = \"Hubens, Nathan\",\n",
    "  journal = \"nathanhubens.github.io\",\n",
    "  year    = \"2020\",\n",
    "  url     = \"https://nathanhubens.github.io/posts/deep%20learning/2020/08/17/FasterAI.html\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- {{'[Cristian Buciluǎ, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, 2006](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf)' | fndetail: 1}}\n",
    "- {{'[Qizhe Xie, Minh-Thang Luong, Eduard Hovy, Quoc V. Le: Self-training with Noisy Student improves ImageNet classification. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020](https://arxiv.org/abs/1911.04252)' | fndetail: 2}}\n",
    "- {{'[H. Li, \"Exploring knowledge distillation of Deep neural nets for efficient hardware solutions,\" CS230 Report, 2018](http://cs230.stanford.edu/files_winter_2018/projects/6940224.pdf)' | fndetail: 3}}\n",
    "- {{'[Zhu, M. & Gupta, S. (2017). To prune, or not to prune: exploring the efficacy of pruning for model compression. ICLR, 2018 ](https://openreview.net/pdf?id=Sy1iIDkPM)' | fndetail: 4}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
