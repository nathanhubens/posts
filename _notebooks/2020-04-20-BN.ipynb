{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed-up inference with Batch Normalization Folding\n",
    "> How to remove the batch normalization layer to make your neural networks faster.\n",
    "\n",
    "- toc: true\n",
    "- badges: false\n",
    "- categories: [Deep Learning]\n",
    "- comments: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "\n",
    "Batch Normalization {% fn 1 %} {% fn 2 %} is a technique which takes care of normalizing the input of each layer to make the training process faster and more stable. In practice, it is an extra layer that we generally add after the computation layer and before the non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "<!-- Create a div where the graph will take place -->\n",
    "<div id=\"my_dataviz\"></div>\n",
    "\n",
    "<style>\n",
    "\n",
    ".axisGray text{\n",
    "  fill: rgb(169,169,179);\n",
    "}  \n",
    "  \n",
    "</style>\n",
    "\n",
    "<!-- Load color palettes -->\n",
    "\n",
    "\n",
    "<script>\n",
    "\n",
    "// set the dimensions and margins of the graph\n",
    "var margin = {top: 80, right: 25, bottom: 30, left: 40},\n",
    "  width = 600 - margin.left - margin.right,\n",
    "  height = 600 - margin.top - margin.bottom;\n",
    "\n",
    "// append the svg object to the body of the page\n",
    "var svg = d3.select(\"#my_dataviz\")\n",
    ".append(\"svg\")\n",
    "  .attr(\"width\", width + margin.left + margin.right)\n",
    "  .attr(\"height\", height + margin.top + margin.bottom)\n",
    ".append(\"g\")\n",
    "  .attr(\"transform\",\n",
    "        \"translate(\" + margin.left + \",\" + margin.top + \")\");\n",
    "\n",
    "//Read the data\n",
    "d3.csv(\"{{site.baseurl}}/assets/csv/out.csv\", function(data) {\n",
    "\n",
    "  // Labels of row and columns -> unique identifier of the column called 'group' and 'variable'\n",
    "  var myGroups = d3.map(data, function(d){return d.group;}).keys()\n",
    "  var myVars = d3.map(data, function(d){return d.variable;}).keys()\n",
    "    \n",
    "    \n",
    "\n",
    "  // Build X scales and axis:\n",
    "  var x = d3.scaleBand()\n",
    "    .range([ 0, width ])\n",
    "    .domain(myGroups)\n",
    "    .padding(0.05);\n",
    "  svg.append(\"g\")\n",
    "    .style(\"font-size\", 15)\n",
    "    .attr(\"class\", \"axisGray\")\n",
    "    .attr(\"transform\", \"translate(0,\" + height + \")\")\n",
    "    .call(d3.axisBottom(x).tickSize(0))\n",
    "    .select(\".domain\").remove()\n",
    "\n",
    "  // Build Y scales and axis:\n",
    "  var y = d3.scaleBand()\n",
    "    .range([ height, 0 ])\n",
    "    .domain(myVars)\n",
    "    .padding(0.05);\n",
    "  svg.append(\"g\")\n",
    "    .style(\"font-size\", 15)\n",
    "    .attr(\"class\", \"axisGray\")\n",
    "    .call(d3.axisLeft(y).tickSize(0))\n",
    "    .select(\".domain\").remove()\n",
    "\n",
    "\n",
    "  // Build color scale\n",
    "  var myColor = d3.scaleSequential()\n",
    "    .interpolator(d3.interpolateInferno)\n",
    "    .domain([1,100])\n",
    "\n",
    "  // create a tooltip\n",
    "  var tooltip = d3.select(\"#my_dataviz\")\n",
    "    .append(\"div\")\n",
    "    .style(\"opacity\", 0)\n",
    "    .attr(\"class\", \"tooltip\")\n",
    "    .style(\"background-color\", \"white\")\n",
    "    .style(\"border\", \"solid\")\n",
    "    .style(\"border-width\", \"2px\")\n",
    "    .style(\"border-radius\", \"5px\")\n",
    "    .style(\"padding\", \"5px\")\n",
    "\n",
    "  // Three function that change the tooltip when user hover / move / leave a cell\n",
    "  var mouseover = function(d) {\n",
    "    tooltip\n",
    "      .style(\"opacity\", 1)\n",
    "    d3.select(this)\n",
    "      .style(\"stroke\", \"black\")\n",
    "      .style(\"opacity\", 1)\n",
    "  }\n",
    "  var mousemove = function(d) {\n",
    "    tooltip\n",
    "      .html(\"Pixel value: \" + d.value)\n",
    "      .style(\"left\", (d3.mouse(this)[0]+70) + \"px\")\n",
    "      .style(\"top\", (d3.mouse(this)[1]) + \"px\")\n",
    "  }\n",
    "  var mouseleave = function(d) {\n",
    "    tooltip\n",
    "      .style(\"opacity\", 0)\n",
    "    d3.select(this)\n",
    "      .style(\"stroke\", \"none\")\n",
    "      .style(\"opacity\", 0.8)\n",
    "  }\n",
    "\n",
    "  // add the squares\n",
    "  svg.selectAll()\n",
    "    .data(data, function(d) {return d.group+':'+d.variable;})\n",
    "    .enter()\n",
    "    .append(\"rect\")\n",
    "      .attr(\"x\", function(d) { return x(d.variable) })\n",
    "      .attr(\"y\", function(d) { return y(d.group) })\n",
    "      .attr(\"rx\", 4)\n",
    "      .attr(\"ry\", 4)\n",
    "      .attr(\"width\", x.bandwidth() )\n",
    "      .attr(\"height\", y.bandwidth() )\n",
    "      .style(\"fill\", function(d) { return myColor(d.value)} )\n",
    "      .style(\"stroke-width\", 4)\n",
    "      .style(\"stroke\", \"none\")\n",
    "      .style(\"opacity\", 0.8)\n",
    "    .on(\"mouseover\", mouseover)\n",
    "    .on(\"mousemove\", mousemove)\n",
    "    .on(\"mouseleave\", mouseleave)\n",
    "})\n",
    "\n",
    "// Add subtitle to graph\n",
    "svg.append(\"text\")\n",
    "        .attr(\"x\", 0)\n",
    "        .attr(\"y\", -20)\n",
    "        .attr(\"text-anchor\", \"left\")\n",
    "        .style(\"font-size\", \"14px\")\n",
    "        .style(\"fill\", \"grey\")\n",
    "        .style(\"max-width\", 400)\n",
    "        .text(\"MNIST visualization\");\n",
    "\n",
    "\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It consists of **2** steps:\n",
    "\n",
    "1. Normalize the batch by first subtracting its mean $\\mu$, then dividing it by its standard deviation $\\sigma$.\n",
    "2. Further scale by a factor $\\gamma$ and shift by a factor $\\beta$. Those are the parameters of the batch normalization layer, required in case of the network not needing the data to have a mean of **0** and a standard deviation of **1**.\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "\\begin{aligned}\n",
    "&\\mu_{\\mathcal{B}} \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} x_{i}\\\\\n",
    "&\\sigma_{\\mathcal{B}}^{2} \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}\\left(x_{i}-\\mu_{\\mathcal{B}}\\right)^{2}\\\\\n",
    "&\\widehat{x}_{i} \\leftarrow \\frac{x_{i}-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^{2}+\\epsilon}}\\\\\n",
    "&y_{i} \\leftarrow \\gamma \\widehat{x}_{i}+\\beta \\equiv \\mathrm{BN}_{\\gamma, \\beta}\\left(x_{i}\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to its efficiency for training neural networks, batch normalization is now widely used. But how useful is it at inference time?\n",
    "\n",
    "Once the training has ended, each batch normalization layer possesses a specific set of $\\gamma$ and $\\beta$, but also $\\mu$ and $\\sigma$, the latter being computed using an exponentially weighted average during training. It means that during inference, the batch normalization acts as a simple linear transformation of what comes out of the previous layer, often a convolution.\n",
    "\n",
    "As a convolution is also a linear transformation, it also means that both operations can be merged into a single linear transformation!\n",
    "\n",
    "This would remove some unnecessary parameters but also reduce the number of operations to be performed at inference time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **How to do that in practice?**\n",
    "\n",
    "\n",
    "With a little bit of math, we can easily rearrange the terms of the convolution to take the batch normalization into account.\n",
    "\n",
    "As a little reminder, the convolution operation followed by the batch normalization operation can be expressed, for an input $x$, as:\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "\\begin{aligned}\n",
    "z &=W * x+b \\\\\n",
    "\\text { out } &=\\gamma \\cdot \\frac{z-\\mu}{\\sqrt{\\sigma^{2}+\\epsilon}}+\\beta\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So, if we re-arrange the $W$ and $b$ of the convolution to take the parameters of the batch normalization into account, as such:\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "\\begin{aligned}\n",
    "w_{\\text {fold }} &=\\gamma \\cdot \\frac{W}{\\sqrt{\\sigma^{2}+\\epsilon}} \\\\\n",
    "b_{\\text {fold }} &=\\gamma \\cdot \\frac{b-\\mu}{\\sqrt{\\sigma^{2}+\\epsilon}}+\\beta\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can remove the batch normalization layer and still have the same results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Usually, you don’t have a bias in a layer preceding a batch normalization layer. It is useless and a waste of parameters as any constant will be canceled out by the batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **How efficient is it?**\n",
    "\n",
    "We will try for **2** common architectures:\n",
    "\n",
    "1. VGG16 with batch norm\n",
    "2. ResNet50\n",
    "\n",
    "Just for the demonstration, we will use ImageNette dataset and PyTorch. Both networks will be trained for **5** epochs and what changes in terms of parameter number and inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **VGG16**\n",
    "\n",
    "Let’s start by training VGG16 for **5** epochs (the final accuracy doesn’t matter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.985012</td>\n",
       "      <td>3.945934</td>\n",
       "      <td>0.226497</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.868819</td>\n",
       "      <td>1.620619</td>\n",
       "      <td>0.472611</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.574975</td>\n",
       "      <td>1.295385</td>\n",
       "      <td>0.576815</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.305211</td>\n",
       "      <td>1.161460</td>\n",
       "      <td>0.617325</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.072395</td>\n",
       "      <td>0.955824</td>\n",
       "      <td>0.684076</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "learn.fit_one_cycle(5, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then show its number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters : 134,309,962\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the initial inference time by using the `%%timeit` magic command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.77 ms ± 1.65 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "model(x[0][None].cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now if we apply batch normalization folding, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters : 134,301,514\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "count_parameters(folded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.41 ms ± 2.49 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "folded_model(x[0][None].cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So **8448** parameters removed and even better, almost **0.4 ms** faster inference! Most importantly, this is completely lossless, there is absolutely no change in terms of performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.9558241, tensor(0.6841)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folded_learner.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see how it behaves in the case of Resnet50!\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Resnet50**\n",
    "\n",
    "Same, we start by training it for **5** epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.076416</td>\n",
       "      <td>2.491038</td>\n",
       "      <td>0.246624</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.696750</td>\n",
       "      <td>1.517581</td>\n",
       "      <td>0.489427</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.313028</td>\n",
       "      <td>1.206347</td>\n",
       "      <td>0.606115</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.057600</td>\n",
       "      <td>0.890211</td>\n",
       "      <td>0.716943</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.828224</td>\n",
       "      <td>0.793130</td>\n",
       "      <td>0.740892</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "learn.fit_one_cycle(5, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial amount of parameters is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters : 23,528,522\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And inference time is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.17 ms ± 13.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "model(x[0][None].cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using batch normalization folding, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters : 23,501,962\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "count_parameters(final_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.47 ms ± 8.97 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "final_model(x[0][None].cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, we have **26,560** parameters removed and even more impressive, an inference time reduce by **1.7ms**! And still without any drop in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.7931296, tensor(0.7409)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_learner.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<span style=\"font-size:larger;\">So if we can reduce the inference time and the number of parameters of our models without enduring any drop in performance, why shouldn’t we always do it?</span>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I hope that this blog post helped you! Feel free to give me feedback or ask me questions is something is not clear enough.**\n",
    "\n",
    "Code available at [this address!](https://github.com/nathanhubens/fasterai)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **References**\n",
    "\n",
    "- {{ '[The Batch Normalization paper](https://arxiv.org/pdf/1502.03167.pdf)' | fndetail: 1 }} \n",
    "- {{ '[DeepLearning.ai Batch Normalization Lesson](https://www.youtube.com/watch?v=tNIpEZLv_eg&t=1s)' | fndetail: 2 }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
